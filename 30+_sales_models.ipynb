{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaL9mW7PJmTciBb+OC4r9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishokramachandiran/ML-projects/blob/main/30%2B_sales_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.PROPHET MODEL"
      ],
      "metadata": {
        "id": "_CR9Ijof80ST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "df=pd.read_csv(input())\n",
        "    # Rename columns to 'ds' and 'y' as required by Prophet\n",
        "df.columns = ['ds', 'y']\n",
        "\n",
        "    # Instantiate the Prophet model\n",
        "model = Prophet()\n",
        "\n",
        "    # Fit the model to the data\n",
        "model.fit(df)\n",
        "\n",
        "    # Create a DataFrame with future dates for the next year\n",
        "future = model.make_future_dataframe(periods=365, freq='D')\n",
        "\n",
        "    # Make predictions for the future dates\n",
        "forecast = model.predict(future)\n",
        "\n",
        "    # Print the forecast table\n",
        "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(365))\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Save the forecast table to a CSV file\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(365).to_csv('prophet_forecast.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('prophet_forecast.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "lZGR-FUZys81",
        "outputId": "df7fb5e4-1000-441a-fa16-bafeb4c35cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp54gf4mcu/uwttgizb.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmp54gf4mcu/gvh6dyr3.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.10/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=7043', 'data', 'file=/tmp/tmp54gf4mcu/uwttgizb.json', 'init=/tmp/tmp54gf4mcu/gvh6dyr3.json', 'output', 'file=/tmp/tmp54gf4mcu/prophet_modelykzf4ceh/prophet_model-20240104103647.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "10:36:47 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "10:36:47 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             ds        yhat  yhat_lower  yhat_upper\n",
            "1826 2024-01-01  306.006682  226.427729  380.803448\n",
            "1827 2024-01-02  300.724505  222.518775  371.580991\n",
            "1828 2024-01-03  307.635297  236.914141  380.740106\n",
            "1829 2024-01-04  301.066232  228.980363  372.993479\n",
            "1830 2024-01-05  308.617891  241.140056  387.406099\n",
            "...         ...         ...         ...         ...\n",
            "2186 2024-12-26  299.556812  225.977708  368.194794\n",
            "2187 2024-12-27  307.220675  238.219728  385.438942\n",
            "2188 2024-12-28  305.318014  232.432507  373.723372\n",
            "2189 2024-12-29  301.068028  227.315935  373.980297\n",
            "2190 2024-12-30  306.283131  235.449025  375.724396\n",
            "\n",
            "[365 rows x 4 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf533343-769a-4ed9-97cd-891b18e70fd1\", \"prophet_forecast.csv\", 24281)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cgSFlBed86gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.***multiple*** ***regression***\n",
        "***"
      ],
      "metadata": {
        "id": "DEF1O4ZqVumX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the data\n",
        "df=pd.read_csv(input())\n",
        "# Convert 'Date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features from 'Date' (e.g., day of the week, month, etc.)\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Set 'Date' as index\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "X = df[['Day', 'Month', 'Year']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Predict sales for the next year\n",
        "# You need to adjust the features accordingly for the future dates\n",
        "# For example, if you have the future dates, extract day, month, and year and use them as features\n",
        "future_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "future_features = pd.DataFrame({\n",
        "    'Day': future_dates.day,\n",
        "    'Month': future_dates.month,\n",
        "    'Year': future_dates.year\n",
        "})\n",
        "\n",
        "# Make predictions for the future\n",
        "future_predictions = model.predict(future_features)\n",
        "\n",
        "# Display the future predictions\n",
        "future_predictions_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Predicted_Sales': future_predictions\n",
        "})\n",
        "\n",
        "print(future_predictions_df)\n",
        "#save the file\n",
        "future_predictions_df.to_csv('MULTIPLE_REG_future_predictions.csv',index=False)\n",
        "from google.colab import files\n",
        "#download file\n",
        "files.download('MULTIPLE_REG_future_predictions.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "SbT2FqpAAMnN",
        "outputId": "c458bde2-fcbf-4fd2-f200-22c73781e945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error: 3364.7119734609446\n",
            "          Date  Predicted_Sales\n",
            "0   2024-01-01       304.216789\n",
            "1   2024-01-02       304.184759\n",
            "2   2024-01-03       304.152730\n",
            "3   2024-01-04       304.120700\n",
            "4   2024-01-05       304.088670\n",
            "..         ...              ...\n",
            "361 2024-12-27       302.844187\n",
            "362 2024-12-28       302.812157\n",
            "363 2024-12-29       302.780128\n",
            "364 2024-12-30       302.748098\n",
            "365 2024-12-31       302.716068\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_58bd4059-07e3-4de7-bca1-0b5250e08168\", \"MULTIPLE_REG_future_predictions.csv\", 10774)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.**DECISON TREE**"
      ],
      "metadata": {
        "id": "amZEO8_lWsnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the data\n",
        "df=pd.read_csv(input())\n",
        "# Convert 'Date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features from 'Date' (e.g., day of the week, month, etc.)\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Set 'Date' as index\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "X = df[['Day', 'Month', 'Year']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree model\n",
        "model = DecisionTreeRegressor()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Predict sales for the next year\n",
        "# You need to adjust the features accordingly for the future dates\n",
        "# For example, if you have the future dates, extract day, month, and year and use them as features\n",
        "future_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "future_features = pd.DataFrame({\n",
        "    'Day': future_dates.day,\n",
        "    'Month': future_dates.month,\n",
        "    'Year': future_dates.year\n",
        "})\n",
        "\n",
        "# Make predictions for the future\n",
        "future_predictions = model.predict(future_features)\n",
        "\n",
        "# Display the future predictions\n",
        "future_predictions_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Predicted_Sales': future_predictions\n",
        "})\n",
        "\n",
        "print(future_predictions_df)\n",
        "\n",
        "\n",
        "future_predictions_df.to_csv('DECISION_TREE_future_predictions.csv',index=False)\n",
        "files.download('DECISION_TREE_future_predictions.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "HlZarzyLAMqI",
        "outputId": "29b21ddf-f081-4ea4-a143-2b6ddc7e140a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error: 6804.571038251366\n",
            "          Date  Predicted_Sales\n",
            "0   2024-01-01            373.0\n",
            "1   2024-01-02            373.0\n",
            "2   2024-01-03            266.0\n",
            "3   2024-01-04            266.0\n",
            "4   2024-01-05            274.0\n",
            "..         ...              ...\n",
            "361 2024-12-27            307.0\n",
            "362 2024-12-28            345.0\n",
            "363 2024-12-29            387.0\n",
            "364 2024-12-30            383.0\n",
            "365 2024-12-31            235.0\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_423e9efe-4b82-4860-a3ba-f829216986e6\", \"DECISION_TREE_future_predictions.csv\", 6243)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.***K MEANS***"
      ],
      "metadata": {
        "id": "ZlklBOQSYLW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the data\n",
        "df=pd.read_csv(input())\n",
        "# Convert 'Date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features from 'Date' (e.g., day of the week, month, etc.)\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "# Set 'Date' as index\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "X = df[['Day', 'Month', 'Year']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a k-Nearest Neighbors model with, for example, k=5 (you can adjust this parameter)\n",
        "model = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "\n",
        "# Predict sales for the next year\n",
        "# You need to adjust the features accordingly for the future dates\n",
        "# For example, if you have the future dates, extract day, month, and year and use them as features\n",
        "future_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "future_features = pd.DataFrame({\n",
        "    'Day': future_dates.day,\n",
        "    'Month': future_dates.month,\n",
        "    'Year': future_dates.year\n",
        "})\n",
        "\n",
        "# Make predictions for the future\n",
        "future_predictions = model.predict(future_features)\n",
        "\n",
        "# Display the future predictions\n",
        "future_predictions_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Predicted_Sales': future_predictions\n",
        "})\n",
        "\n",
        "print(future_predictions_df)\n",
        "\n",
        "\n",
        "future_predictions_df.to_csv('kmeans_future_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('kmeans_future_predictions_df.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "UQ6BJwYHAMs3",
        "outputId": "d2ab8cb6-f8cf-4d35-d1c8-60c02744ebb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error: 4015.5378142076506\n",
            "          Date  Predicted_Sales\n",
            "0   2024-01-01            343.2\n",
            "1   2024-01-02            349.2\n",
            "2   2024-01-03            348.4\n",
            "3   2024-01-04            308.2\n",
            "4   2024-01-05            290.4\n",
            "..         ...              ...\n",
            "361 2024-12-27            314.4\n",
            "362 2024-12-28            364.4\n",
            "363 2024-12-29            368.2\n",
            "364 2024-12-30            346.2\n",
            "365 2024-12-31            288.8\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64bb6271-b9d3-44dd-b3c7-19841162bb42\", \"kmeans_future_predictions_df.csv\", 6243)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.**logestic** regression"
      ],
      "metadata": {
        "id": "eb95E0EOirst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your sales data\n",
        "# Assuming you have a CSV file named 'sales_data.csv' with columns 'Date' and 'Sales'\n",
        "sales_data = pd.read_csv(input())\n",
        "\n",
        "# Extract features (date) and target variable (sales)\n",
        "X = pd.to_numeric(pd.to_datetime(sales_data['Date'])).values.reshape(-1, 1)\n",
        "y = sales_data['Sales']\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize and train the linear regression model on the full dataset\n",
        "model = LinearRegression()\n",
        "model.fit(X_scaled, y)\n",
        "\n",
        "# Predict sales for the next year\n",
        "# Assuming 'new_dates' is a list of dates for the next year\n",
        "new_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "new_dates_numeric = pd.to_numeric(new_dates).values.reshape(-1, 1)\n",
        "new_dates_scaled = scaler.transform(new_dates_numeric)\n",
        "\n",
        "# Predict sales for the new dates\n",
        "new_sales_pred = model.predict(new_dates_scaled)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "predictions_df = pd.DataFrame({'Date': new_dates, 'Predicted Sales': new_sales_pred})\n",
        "\n",
        "# Print the table\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('log_reg_predictions_df.csv',index=False)\n",
        "files.download('log_reg_predictions_df.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "oKrhfSVIAM11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "96bca4ae-d86c-4021-81e6-77b27d17bf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "          Date  Predicted Sales\n",
            "0   2024-01-01       302.915512\n",
            "1   2024-01-02       302.917133\n",
            "2   2024-01-03       302.918754\n",
            "3   2024-01-04       302.920375\n",
            "4   2024-01-05       302.921996\n",
            "..         ...              ...\n",
            "361 2024-12-27       303.500653\n",
            "362 2024-12-28       303.502274\n",
            "363 2024-12-29       303.503895\n",
            "364 2024-12-30       303.505515\n",
            "365 2024-12-31       303.507136\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f4f5bd3-6d0c-4674-8616-7dde8c88c64d\", \"log_reg_predictions_df.csv\", 10768)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.***support vector regression(svr)***"
      ],
      "metadata": {
        "id": "RLOjts3xjV4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your sales data\n",
        "# Assuming you have a CSV file named 'sales_data.csv' with columns 'Date' and 'Sales'\n",
        "sales_data = pd.read_csv(input())\n",
        "# Extract features (date) and target variable (sales)\n",
        "X = pd.to_numeric(pd.to_datetime(sales_data['Date'])).values.reshape(-1, 1)\n",
        "y = sales_data['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Support Vector Regression model\n",
        "model = SVR()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Predict sales for the next year\n",
        "# Assuming 'new_dates' is a list of dates for the next year\n",
        "new_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "new_dates_numeric = pd.to_numeric(new_dates).values.reshape(-1, 1)\n",
        "new_dates_scaled = scaler.transform(new_dates_numeric)\n",
        "\n",
        "# Predict sales for the new dates\n",
        "new_sales_pred = model.predict(new_dates_scaled)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "predictions_df = pd.DataFrame({'Date': new_dates, 'Predicted Sales': new_sales_pred})\n",
        "\n",
        "# Print the table\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('svr_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('svr_predictions_df.csv')\n"
      ],
      "metadata": {
        "id": "_quUgPKvAM4p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "48251552-8a8e-4eb7-e2fe-4b67eee2bb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error: 3408.428332116121\n",
            "          Date  Predicted Sales\n",
            "0   2024-01-01       305.758913\n",
            "1   2024-01-02       305.765997\n",
            "2   2024-01-03       305.773073\n",
            "3   2024-01-04       305.780138\n",
            "4   2024-01-05       305.787194\n",
            "..         ...              ...\n",
            "361 2024-12-27       307.010114\n",
            "362 2024-12-28       307.009455\n",
            "363 2024-12-29       307.008777\n",
            "364 2024-12-30       307.008081\n",
            "365 2024-12-31       307.007367\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a844972a-0c6d-4ede-880b-9989c1b3bd16\", \"svr_predictions_df.csv\", 10753)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.***Gradient boosting machines (gbm)***"
      ],
      "metadata": {
        "id": "RJnIxS_8krQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your sales data\n",
        "# Assuming you have a CSV file named 'sales_data.csv' with columns 'Date' and 'Sales'\n",
        "\n",
        "sales_data = pd.read_csv(input())\n",
        "\n",
        "# Extract features (date) and target variable (sales)\n",
        "X = pd.to_numeric(pd.to_datetime(sales_data['Date'])).values.reshape(-1, 1)\n",
        "y = sales_data['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize and train the Gradient Boosting Regressor model\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Predict sales for the next year\n",
        "# Assuming 'new_dates' is a list of dates for the next year\n",
        "new_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')\n",
        "new_dates_numeric = pd.to_numeric(new_dates).values.reshape(-1, 1)\n",
        "new_dates_scaled = scaler.transform(new_dates_numeric)\n",
        "\n",
        "# Predict sales for the new dates\n",
        "new_sales_pred = model.predict(new_dates_scaled)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "predictions_df = pd.DataFrame({'Date': new_dates, 'Predicted Sales': new_sales_pred})\n",
        "\n",
        "# Print the table\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('gbm_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('gbm_predictions_df.csv')"
      ],
      "metadata": {
        "id": "z__pp2fVAM7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "1cc1d7fc-861f-47ec-9bc0-3bdf7c7b3c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error: 3467.2196220301466\n",
            "          Date  Predicted Sales\n",
            "0   2024-01-01       257.020223\n",
            "1   2024-01-02       257.020223\n",
            "2   2024-01-03       257.020223\n",
            "3   2024-01-04       257.020223\n",
            "4   2024-01-05       257.020223\n",
            "..         ...              ...\n",
            "361 2024-12-27       257.020223\n",
            "362 2024-12-28       257.020223\n",
            "363 2024-12-29       257.020223\n",
            "364 2024-12-30       257.020223\n",
            "365 2024-12-31       257.020223\n",
            "\n",
            "[366 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f1ef72b-834a-4c66-aee4-c03d5fa0f9cf\", \"gbm_predictions_df.csv\", 10635)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.***gru(Gated Recurrent Unit)***"
      ],
      "metadata": {
        "id": "NVYPwSuwHWoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "\n",
        "# Read data from CSV file\n",
        "file_path = input()\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming you have a 'Date' column in your data\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Set 'Date' column as the index of the DataFrame\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Extract the 'Sales' column as a NumPy array\n",
        "sales_data = df['Sales'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "sales_data_normalized = scaler.fit_transform(sales_data)\n",
        "\n",
        "# Define a function to create input sequences for the GRU model\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    for i in range(len(data) - seq_length + 1):\n",
        "        seq = data[i:i + seq_length, 0]\n",
        "        label = data[i + seq_length - 1, 0]\n",
        "        sequences.append((seq, label))\n",
        "    return np.array(sequences)\n",
        "\n",
        "# Set the sequence length (adjust as needed)\n",
        "sequence_length = 10\n",
        "\n",
        "# Create input sequences and labels\n",
        "sequences = create_sequences(sales_data_normalized, sequence_length)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_sequences, test_sequences = sequences[:train_size], sequences[train_size:]\n",
        "\n",
        "X_train = np.array([seq[0] for seq in train_sequences])\n",
        "y_train = np.array([seq[1] for seq in train_sequences])\n",
        "\n",
        "X_test = np.array([seq[0] for seq in test_sequences])\n",
        "y_test = np.array([seq[1] for seq in test_sequences])\n",
        "\n",
        "# Build the GRU model\n",
        "model = Sequential([\n",
        "    GRU(units=50, activation='tanh', input_shape=(X_train.shape[1], 1)),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict using the trained model\n",
        "predicted_sales_normalized = model.predict(X_test)\n",
        "\n",
        "# Invert the normalization\n",
        "predicted_sales = scaler.inverse_transform(predicted_sales_normalized.reshape(-1, 1))\n",
        "\n",
        "# Print the predicted sales\n",
        "print(\"Predicted Sales:\")\n",
        "print(predicted_sales)\n",
        "\n",
        "# Convert the predicted sales array to a DataFrame with dates\n",
        "predicted_sales_df = pd.DataFrame(data={'Date': df.index[-len(predicted_sales):], 'Predicted_Sales': predicted_sales.flatten()})\n",
        "\n",
        "# Save the predicted sales DataFrame to a CSV file\n",
        "predicted_sales_df.to_csv('gru_predicted_sales.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "files.download('gru_predicted_sales.csv')\n"
      ],
      "metadata": {
        "id": "pw0TyHWNANkA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9e69563-8bb7-47aa-f846-62ed5dc463a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-1446363bcf54>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(sequences)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "46/46 [==============================] - 3s 19ms/step - loss: 0.0983 - val_loss: 0.0582\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0482 - val_loss: 0.0363\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0282 - val_loss: 0.0166\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0019\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.0656e-04 - val_loss: 1.6927e-04\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.5805e-04 - val_loss: 7.2492e-05\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.3653e-05 - val_loss: 3.2711e-05\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.6804e-05 - val_loss: 2.1267e-05\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 2.4942e-05 - val_loss: 1.6104e-05\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 0s 11ms/step - loss: 1.7970e-05 - val_loss: 1.2854e-05\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.8445e-05 - val_loss: 1.9559e-05\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.9871e-05 - val_loss: 2.5178e-05\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.6932e-05 - val_loss: 1.3910e-05\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 1s 13ms/step - loss: 1.5763e-05 - val_loss: 3.4248e-05\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 1.3740e-05 - val_loss: 1.1123e-05\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 0s 10ms/step - loss: 1.2234e-05 - val_loss: 9.2484e-06\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.2986e-05 - val_loss: 7.5559e-06\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 1.2627e-05 - val_loss: 7.6124e-06\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 8.8319e-06 - val_loss: 1.3846e-05\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.0217e-06 - val_loss: 6.2902e-06\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 9.3436e-06 - val_loss: 7.0999e-06\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.3819e-06 - val_loss: 8.7209e-06\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 8.9596e-06 - val_loss: 7.6297e-06\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 8.3595e-06 - val_loss: 5.8967e-06\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.9372e-06 - val_loss: 1.9097e-05\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 9.0842e-06 - val_loss: 6.0865e-06\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.9869e-06 - val_loss: 4.3584e-06\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.3725e-06 - val_loss: 4.1813e-06\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 7.9542e-06 - val_loss: 8.9863e-06\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 7.0778e-06 - val_loss: 9.3301e-06\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.2295e-06 - val_loss: 3.7390e-06\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.1070e-06 - val_loss: 6.7233e-06\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 1s 16ms/step - loss: 6.1738e-06 - val_loss: 5.1815e-06\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 1s 19ms/step - loss: 4.8236e-06 - val_loss: 1.1514e-05\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 5.7897e-06 - val_loss: 6.3515e-06\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 1s 17ms/step - loss: 5.6036e-06 - val_loss: 5.5123e-06\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 7.5743e-06 - val_loss: 4.8389e-06\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 1s 24ms/step - loss: 4.8001e-06 - val_loss: 7.5697e-06\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 5.3500e-06 - val_loss: 3.1250e-06\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 1s 25ms/step - loss: 7.0596e-06 - val_loss: 3.1440e-06\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 1s 20ms/step - loss: 3.5835e-06 - val_loss: 6.0520e-06\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 1s 12ms/step - loss: 3.9658e-06 - val_loss: 4.8002e-06\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.5870e-06 - val_loss: 4.9057e-06\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 6.2160e-06 - val_loss: 2.5626e-06\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 4.1652e-06 - val_loss: 6.8681e-06\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 4.7466e-06 - val_loss: 1.0308e-05\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 6.1718e-06 - val_loss: 3.1277e-06\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 5.9853e-06 - val_loss: 9.9479e-06\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 3.3745e-06 - val_loss: 2.4197e-06\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 0s 8ms/step - loss: 2.9986e-06 - val_loss: 6.8466e-06\n",
            "12/12 [==============================] - 1s 5ms/step\n",
            "Predicted Sales:\n",
            "[[373.2114 ]\n",
            " [266.46786]\n",
            " [225.11635]\n",
            " [274.4433 ]\n",
            " [378.38528]\n",
            " [349.47205]\n",
            " [379.1663 ]\n",
            " [277.67722]\n",
            " [395.21283]\n",
            " [299.8436 ]\n",
            " [300.71655]\n",
            " [385.2121 ]\n",
            " [304.83972]\n",
            " [360.67523]\n",
            " [345.8241 ]\n",
            " [216.35313]\n",
            " [320.69717]\n",
            " [364.46713]\n",
            " [309.70596]\n",
            " [264.47433]\n",
            " [216.03484]\n",
            " [332.61502]\n",
            " [261.39783]\n",
            " [321.74567]\n",
            " [285.57788]\n",
            " [338.7182 ]\n",
            " [270.54593]\n",
            " [208.19292]\n",
            " [374.14304]\n",
            " [271.3506 ]\n",
            " [399.93887]\n",
            " [255.13182]\n",
            " [395.88684]\n",
            " [389.74832]\n",
            " [215.08983]\n",
            " [328.2722 ]\n",
            " [273.2214 ]\n",
            " [360.4597 ]\n",
            " [336.44403]\n",
            " [390.97717]\n",
            " [229.24634]\n",
            " [322.63937]\n",
            " [356.53223]\n",
            " [206.97572]\n",
            " [212.8412 ]\n",
            " [356.25876]\n",
            " [273.17633]\n",
            " [277.3548 ]\n",
            " [313.56067]\n",
            " [321.6565 ]\n",
            " [336.6936 ]\n",
            " [390.14307]\n",
            " [274.57504]\n",
            " [353.69815]\n",
            " [315.93967]\n",
            " [306.92813]\n",
            " [280.78824]\n",
            " [330.88162]\n",
            " [250.46858]\n",
            " [223.19734]\n",
            " [347.6633 ]\n",
            " [300.63223]\n",
            " [277.5761 ]\n",
            " [312.74384]\n",
            " [252.46683]\n",
            " [252.51248]\n",
            " [289.78137]\n",
            " [399.10568]\n",
            " [257.30872]\n",
            " [383.26825]\n",
            " [298.49838]\n",
            " [225.12694]\n",
            " [371.24722]\n",
            " [290.35428]\n",
            " [380.17053]\n",
            " [289.4934 ]\n",
            " [218.02321]\n",
            " [336.49744]\n",
            " [369.3121 ]\n",
            " [264.38083]\n",
            " [269.3712 ]\n",
            " [212.99791]\n",
            " [308.5956 ]\n",
            " [378.33517]\n",
            " [370.2138 ]\n",
            " [342.5598 ]\n",
            " [385.33136]\n",
            " [329.04965]\n",
            " [359.73386]\n",
            " [303.00336]\n",
            " [224.40396]\n",
            " [240.3353 ]\n",
            " [228.21129]\n",
            " [358.61115]\n",
            " [204.71773]\n",
            " [311.36697]\n",
            " [274.40866]\n",
            " [361.52173]\n",
            " [310.58786]\n",
            " [322.59607]\n",
            " [217.13707]\n",
            " [262.32104]\n",
            " [400.00076]\n",
            " [344.344  ]\n",
            " [305.63147]\n",
            " [354.57425]\n",
            " [365.58536]\n",
            " [215.29303]\n",
            " [393.99332]\n",
            " [328.34674]\n",
            " [290.5694 ]\n",
            " [336.6311 ]\n",
            " [269.50308]\n",
            " [376.4065 ]\n",
            " [356.50775]\n",
            " [328.86377]\n",
            " [236.41801]\n",
            " [393.2839 ]\n",
            " [287.59454]\n",
            " [382.29865]\n",
            " [210.9729 ]\n",
            " [224.84949]\n",
            " [230.99162]\n",
            " [233.32376]\n",
            " [265.69034]\n",
            " [342.8399 ]\n",
            " [269.41037]\n",
            " [388.22867]\n",
            " [379.00168]\n",
            " [369.0777 ]\n",
            " [250.35799]\n",
            " [327.54257]\n",
            " [310.70844]\n",
            " [225.28122]\n",
            " [261.37704]\n",
            " [307.63647]\n",
            " [301.5883 ]\n",
            " [234.21596]\n",
            " [388.2475 ]\n",
            " [231.03308]\n",
            " [332.52502]\n",
            " [344.4401 ]\n",
            " [321.5498 ]\n",
            " [369.40598]\n",
            " [313.79645]\n",
            " [365.6402 ]\n",
            " [206.2071 ]\n",
            " [208.98929]\n",
            " [328.4678 ]\n",
            " [315.6069 ]\n",
            " [330.5635 ]\n",
            " [353.4279 ]\n",
            " [283.5417 ]\n",
            " [328.74585]\n",
            " [300.75546]\n",
            " [325.71863]\n",
            " [258.58206]\n",
            " [391.3626 ]\n",
            " [297.68924]\n",
            " [361.6652 ]\n",
            " [337.8239 ]\n",
            " [259.664  ]\n",
            " [215.21042]\n",
            " [212.1006 ]\n",
            " [288.62796]\n",
            " [297.82584]\n",
            " [241.3066 ]\n",
            " [321.74133]\n",
            " [263.51508]\n",
            " [344.76877]\n",
            " [347.6404 ]\n",
            " [372.29205]\n",
            " [394.97577]\n",
            " [226.5549 ]\n",
            " [274.52316]\n",
            " [312.6961 ]\n",
            " [264.56876]\n",
            " [293.62943]\n",
            " [253.2892 ]\n",
            " [250.3172 ]\n",
            " [219.22285]\n",
            " [258.64325]\n",
            " [278.18713]\n",
            " [324.1291 ]\n",
            " [250.7161 ]\n",
            " [373.5045 ]\n",
            " [232.2601 ]\n",
            " [265.386  ]\n",
            " [316.67432]\n",
            " [263.5925 ]\n",
            " [353.6444 ]\n",
            " [300.5962 ]\n",
            " [303.71994]\n",
            " [300.7465 ]\n",
            " [262.62213]\n",
            " [205.26425]\n",
            " [389.08533]\n",
            " [301.39984]\n",
            " [392.96713]\n",
            " [243.1485 ]\n",
            " [271.28378]\n",
            " [209.99982]\n",
            " [299.45258]\n",
            " [376.27936]\n",
            " [331.3937 ]\n",
            " [294.50986]\n",
            " [307.66815]\n",
            " [369.52667]\n",
            " [286.6374 ]\n",
            " [376.41577]\n",
            " [346.67276]\n",
            " [353.90097]\n",
            " [273.95468]\n",
            " [280.8588 ]\n",
            " [314.90518]\n",
            " [239.42456]\n",
            " [311.767  ]\n",
            " [215.07976]\n",
            " [301.59567]\n",
            " [353.61636]\n",
            " [256.3752 ]\n",
            " [397.99213]\n",
            " [336.4219 ]\n",
            " [224.11432]\n",
            " [394.77902]\n",
            " [279.2733 ]\n",
            " [359.32373]\n",
            " [268.34995]\n",
            " [388.11707]\n",
            " [351.42456]\n",
            " [240.34569]\n",
            " [302.61243]\n",
            " [300.62213]\n",
            " [209.0498 ]\n",
            " [227.09668]\n",
            " [267.4965 ]\n",
            " [295.85394]\n",
            " [291.74365]\n",
            " [321.89233]\n",
            " [303.85812]\n",
            " [266.66235]\n",
            " [323.80063]\n",
            " [233.48715]\n",
            " [398.01086]\n",
            " [348.3939 ]\n",
            " [260.4356 ]\n",
            " [267.44928]\n",
            " [366.49164]\n",
            " [306.62778]\n",
            " [342.65335]\n",
            " [260.47278]\n",
            " [363.5559 ]\n",
            " [274.55722]\n",
            " [335.81116]\n",
            " [395.98917]\n",
            " [317.8801 ]\n",
            " [359.79407]\n",
            " [345.9399 ]\n",
            " [339.09195]\n",
            " [205.34486]\n",
            " [295.6495 ]\n",
            " [393.04868]\n",
            " [357.38596]\n",
            " [355.5562 ]\n",
            " [316.86023]\n",
            " [344.88934]\n",
            " [318.01578]\n",
            " [234.50198]\n",
            " [326.6734 ]\n",
            " [324.71933]\n",
            " [368.49448]\n",
            " [230.26442]\n",
            " [336.68198]\n",
            " [205.93169]\n",
            " [390.06287]\n",
            " [221.70538]\n",
            " [203.63248]\n",
            " [263.04752]\n",
            " [230.34377]\n",
            " [338.68246]\n",
            " [282.7869 ]\n",
            " [355.5776 ]\n",
            " [384.03183]\n",
            " [375.23785]\n",
            " [347.46503]\n",
            " [212.19342]\n",
            " [228.11691]\n",
            " [241.3248 ]\n",
            " [265.62988]\n",
            " [332.75922]\n",
            " [305.47495]\n",
            " [263.3217 ]\n",
            " [235.29291]\n",
            " [382.22195]\n",
            " [268.501  ]\n",
            " [271.47504]\n",
            " [354.51593]\n",
            " [344.55856]\n",
            " [329.75537]\n",
            " [339.7626 ]\n",
            " [322.85562]\n",
            " [238.51822]\n",
            " [249.5183 ]\n",
            " [312.8158 ]\n",
            " [245.43765]\n",
            " [395.17496]\n",
            " [260.16025]\n",
            " [258.27414]\n",
            " [241.25409]\n",
            " [341.6379 ]\n",
            " [263.5108 ]\n",
            " [270.52737]\n",
            " [387.20514]\n",
            " [359.32693]\n",
            " [255.4424 ]\n",
            " [224.10957]\n",
            " [335.48303]\n",
            " [276.52545]\n",
            " [396.07535]\n",
            " [364.20993]\n",
            " [391.0499 ]\n",
            " [361.8377 ]\n",
            " [258.89938]\n",
            " [258.51227]\n",
            " [343.70813]\n",
            " [269.55917]\n",
            " [288.56436]\n",
            " [251.20532]\n",
            " [296.62607]\n",
            " [240.3277 ]\n",
            " [308.82852]\n",
            " [387.2974 ]\n",
            " [339.59164]\n",
            " [227.2478 ]\n",
            " [323.52692]\n",
            " [317.6505 ]\n",
            " [248.39545]\n",
            " [334.6675 ]\n",
            " [306.5993 ]\n",
            " [328.73932]\n",
            " [210.05757]\n",
            " [283.48337]\n",
            " [343.6812 ]\n",
            " [220.12283]\n",
            " [354.4854 ]\n",
            " [200.80235]\n",
            " [310.35333]\n",
            " [385.99835]\n",
            " [281.28577]\n",
            " [386.9284 ]\n",
            " [376.07776]\n",
            " [295.759  ]\n",
            " [240.30325]\n",
            " [393.12924]\n",
            " [264.2786 ]\n",
            " [329.61945]\n",
            " [382.08945]\n",
            " [218.02168]\n",
            " [315.49048]\n",
            " [307.52472]\n",
            " [345.607  ]\n",
            " [387.073  ]\n",
            " [383.29514]\n",
            " [235.5584 ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a84f51e-a199-4be7-afc2-f33bf104fd72\", \"gru_predicted_sales.csv\", 7549)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.**Ridge regession**"
      ],
      "metadata": {
        "id": "53qxXmmyfLKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load your CSV file\n",
        "file_path = input()\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features (days since the first date) and target variable\n",
        "df['Days'] = (df['Date'] - df['Date'].min()).dt.days\n",
        "X = df[['Days']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha parameter as needed\n",
        "\n",
        "# Train the model\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error on Test Set: {mse}')\n",
        "\n",
        "# Now, to make predictions for the next year, you need to prepare the features for the next year\n",
        "# Let's assume you want to predict sales for the next 365 days\n",
        "last_date = df['Date'].max()\n",
        "next_year_dates = pd.date_range(last_date + timedelta(days=1), periods=365, freq='D')\n",
        "X_next_year = pd.DataFrame({'Days': (next_year_dates - df['Date'].min()).days})\n",
        "\n",
        "# Standardize the features for prediction\n",
        "X_next_year_scaled = scaler.transform(X_next_year)\n",
        "\n",
        "# Predict sales for the next year\n",
        "predictions_next_year = ridge_model.predict(X_next_year_scaled)\n",
        "\n",
        "# Print the predicted sales for the next year\n",
        "next_year_predictions_df = pd.DataFrame({'Date': next_year_dates, 'Predicted_Sales': predictions_next_year})\n",
        "print('Predicted Sales for the Next Year:')\n",
        "print(next_year_predictions_df)\n",
        "\n",
        "\n",
        "next_year_predictions_df.to_csv('ridge_reg_next_year_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('ridge_reg_next_year_predictions_df.csv')\n"
      ],
      "metadata": {
        "id": "LStqLYU874OE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f4dbfb9a-d736-4836-bdc9-8b011241b1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error on Test Set: 3363.502149795281\n",
            "Predicted Sales for the Next Year:\n",
            "          Date  Predicted_Sales\n",
            "0   2024-01-01       303.266242\n",
            "1   2024-01-02       303.266791\n",
            "2   2024-01-03       303.267340\n",
            "3   2024-01-04       303.267889\n",
            "4   2024-01-05       303.268438\n",
            "..         ...              ...\n",
            "360 2024-12-26       303.463874\n",
            "361 2024-12-27       303.464423\n",
            "362 2024-12-28       303.464972\n",
            "363 2024-12-29       303.465521\n",
            "364 2024-12-30       303.466070\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4a074cca-bd13-46bf-a26f-68ffdca4a2f0\", \"ridge_reg_next_year_predictions_df.csv\", 10744)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Lasso *regression*"
      ],
      "metadata": {
        "id": "EXw_idtEiP0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Load your CSV file\n",
        "file_path = input()\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'Date' column to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features (days since the first date) and target variable\n",
        "df['Days'] = (df['Date'] - df['Date'].min()).dt.days\n",
        "X = df[['Days']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Lasso Regression model with a lower alpha value\n",
        "lasso_model = Lasso(alpha=0.01)  # You can further adjust the alpha parameter\n",
        "\n",
        "# Train the model\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error on Test Set: {mse}')\n",
        "\n",
        "# Now, to make predictions for the next year, you need to prepare the features for the next year\n",
        "# Let's assume you want to predict sales for the next 365 days\n",
        "last_date = df['Date'].max()\n",
        "next_year_dates = pd.date_range(last_date + timedelta(days=1), periods=365, freq='D')\n",
        "X_next_year = pd.DataFrame({'Days': (next_year_dates - df['Date'].min()).days})\n",
        "\n",
        "# Standardize the features for prediction\n",
        "X_next_year_scaled = scaler.transform(X_next_year)\n",
        "\n",
        "# Predict sales for the next year\n",
        "predictions_next_year = lasso_model.predict(X_next_year_scaled)\n",
        "\n",
        "# Print the predicted sales for the next year\n",
        "next_year_predictions_df = pd.DataFrame({'Date': next_year_dates, 'Predicted_Sales': predictions_next_year})\n",
        "print('Predicted Sales for the Next Year:')\n",
        "print(next_year_predictions_df)\n",
        "\n",
        "\n",
        "next_year_predictions_df.to_csv('lasso_reg_next_year_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('lasso_reg_next_year_predictions_df.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "q4sKqxSshIvy",
        "outputId": "022aa2e7-11bd-4231-f72e-9be943e64e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Mean Squared Error on Test Set: 3363.556475978738\n",
            "Predicted Sales for the Next Year:\n",
            "          Date  Predicted_Sales\n",
            "0   2024-01-01       303.249269\n",
            "1   2024-01-02       303.249799\n",
            "2   2024-01-03       303.250330\n",
            "3   2024-01-04       303.250860\n",
            "4   2024-01-05       303.251391\n",
            "..         ...              ...\n",
            "360 2024-12-26       303.440235\n",
            "361 2024-12-27       303.440766\n",
            "362 2024-12-28       303.441296\n",
            "363 2024-12-29       303.441826\n",
            "364 2024-12-30       303.442357\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a83148e1-b91f-421a-ade0-03c7f3b3a76a\", \"lasso_reg_next_year_predictions_df.csv\", 10741)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.***SVR***"
      ],
      "metadata": {
        "id": "ocwvDah_qW49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVR Model\n",
        "svr_model = SVR(kernel='rbf')\n",
        "svr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = svr_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "\n",
        "\n",
        "predictions_df.to_csv('svr_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('svr_predictions_df.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "SyzTdN7vmTQL",
        "outputId": "82792201-ca73-46e4-fed9-20a9408d1798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      306.826985\n",
            "1   2024-01-01      309.602531\n",
            "2   2024-01-02      309.584546\n",
            "3   2024-01-03      309.565612\n",
            "4   2024-01-04      309.545727\n",
            "..         ...             ...\n",
            "360 2024-12-25      306.689865\n",
            "361 2024-12-26      306.717863\n",
            "362 2024-12-27      306.745589\n",
            "363 2024-12-28      306.773029\n",
            "364 2024-12-29      306.800166\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e72b63ca-bede-4350-a2ba-fdc896effa7a\", \"svr_predictions_df.csv\", 10738)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.MLP regressor"
      ],
      "metadata": {
        "id": "gSm3v6ENqp11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the MLPRegressor model\n",
        "mlp_model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = mlp_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('mlp_reg_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('mlp_reg_predictions_df.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "0K7pdvFjme0o",
        "outputId": "7a632d36-dfc1-490c-a45a-fd53fe503788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      313.565994\n",
            "1   2024-01-01      310.878676\n",
            "2   2024-01-02      310.139584\n",
            "3   2024-01-03      309.484326\n",
            "4   2024-01-04      309.165341\n",
            "..         ...             ...\n",
            "360 2024-12-25      309.737160\n",
            "361 2024-12-26      310.502927\n",
            "362 2024-12-27      311.268693\n",
            "363 2024-12-28      312.034460\n",
            "364 2024-12-29      312.800227\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86e65787-130b-4587-b12f-c04cb267a846\", \"mlp_reg_predictions_df.csv\", 10730)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.**ARD REGRESSION**"
      ],
      "metadata": {
        "id": "mo-gRcAUrB7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import ARDRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the ARDRegression model\n",
        "ard_model = ARDRegression()\n",
        "ard_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = ard_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('ard_reg_predictions_df.csv',index=False)\n",
        "from google.colab import files\n",
        "files.download('ard_reg_predictions_df.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "-Y21L6s0mevP",
        "outputId": "2f0b4c98-5e99-4b47-ddfa-1476c2725bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      302.761877\n",
            "1   2024-01-01      302.764150\n",
            "2   2024-01-02      302.764144\n",
            "3   2024-01-03      302.764138\n",
            "4   2024-01-04      302.764132\n",
            "..         ...             ...\n",
            "360 2024-12-25      302.761908\n",
            "361 2024-12-26      302.761902\n",
            "362 2024-12-27      302.761896\n",
            "363 2024-12-28      302.761889\n",
            "364 2024-12-29      302.761883\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c4061814-5db2-445f-af03-7ba45b0673fe\", \"ard_reg_predictions_df.csv\", 10740)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.***Stacking regressor***"
      ],
      "metadata": {
        "id": "pDUT8omeriHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define individual regressor models\n",
        "linear_reg = LinearRegression()\n",
        "mlp_reg = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "svr_reg = SVR(kernel='rbf')\n",
        "\n",
        "# Create a StackingRegressor with Linear Regression as the final estimator\n",
        "stacked_model = StackingRegressor(\n",
        "    estimators=[('linear', linear_reg), ('mlp', mlp_reg), ('svr', svr_reg)],\n",
        "    final_estimator=LinearRegression()\n",
        ")\n",
        "\n",
        "# Train the StackingRegressor model\n",
        "stacked_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = stacked_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('stack_reg_predictions_df.csv',index=False)\n",
        "\n",
        "\n",
        "files.download('stack_reg_predictions_df.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "nKnKIgjNrWLM",
        "outputId": "ff483621-3f37-4d83-fef4-ee44a3bca799"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      299.071078\n",
            "1   2024-01-01      294.181772\n",
            "2   2024-01-02      294.330288\n",
            "3   2024-01-03      294.467006\n",
            "4   2024-01-04      294.551837\n",
            "..         ...             ...\n",
            "360 2024-12-25      299.885835\n",
            "361 2024-12-26      299.721968\n",
            "362 2024-12-27      299.558534\n",
            "363 2024-12-28      299.395557\n",
            "364 2024-12-29      299.233064\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d25383f3-a2eb-478f-b707-23e87b648740\", \"stack_reg_predictions_df.csv\", 10744)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.***PLS REGRESSION***"
      ],
      "metadata": {
        "id": "7DLLzv6A-0aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the LinearRegression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = linear_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions.flatten()})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "\n",
        "predictions_df.to_csv('pls_reg_predictions_df.csv',index=False)\n",
        "\n",
        "files.download('pls_reg_predictions_df.csv')\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "4nleAblnrmfj",
        "outputId": "989fea35-b18c-47fc-c08f-dabe7664c16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      302.432201\n",
            "1   2024-01-01      303.093776\n",
            "2   2024-01-02      303.091959\n",
            "3   2024-01-03      303.090141\n",
            "4   2024-01-04      303.088324\n",
            "..         ...             ...\n",
            "360 2024-12-25      302.441289\n",
            "361 2024-12-26      302.439472\n",
            "362 2024-12-27      302.437654\n",
            "363 2024-12-28      302.435836\n",
            "364 2024-12-29      302.434019\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7d175ed-49ff-47c1-a9c5-db31184da33b\", \"pls_reg_predictions_df.csv\", 10731)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.**poisson regressor**"
      ],
      "metadata": {
        "id": "2h0rzss6GnCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the PoissonRegressor model\n",
        "poisson_model = PoissonRegressor()\n",
        "poisson_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = poisson_model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predictions_df.to_csv('poisson_reg_predictions_df.csv',index=False)\n",
        "from google.colab import files\n",
        "\n",
        "files.download('poisson_reg_predictions_df.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "KEY1SYK-_xuW",
        "outputId": "2cd6a338-6c78-4e73-b0e7-b52613cdd509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      302.433407\n",
            "1   2024-01-01      303.092805\n",
            "2   2024-01-02      303.090992\n",
            "3   2024-01-03      303.089178\n",
            "4   2024-01-04      303.087365\n",
            "..         ...             ...\n",
            "360 2024-12-25      302.442455\n",
            "361 2024-12-26      302.440645\n",
            "362 2024-12-27      302.438835\n",
            "363 2024-12-28      302.437026\n",
            "364 2024-12-29      302.435216\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b25fe06b-7ab3-4ee1-95fd-d5f9ec8cb9fc\", \"poisson_reg_predictions_df.csv\", 10751)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.LSTM###########\n"
      ],
      "metadata": {
        "id": "awqMe_bPDT6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load data (consider downloading and saving the data locally)\n",
        "data_url = input()\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "data = df[['NumericDate', 'Sales']].values\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Define constants\n",
        "SEQUENCE_LENGTH = 10\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 32\n",
        "FUTURE_STEPS = 365  # Number of days to predict\n",
        "\n",
        "# Create sequences\n",
        "sequences = create_sequences(data_scaled, SEQUENCE_LENGTH)\n",
        "\n",
        "# Split into training and testing sets\n",
        "train_size = int(len(sequences) * 0.8)\n",
        "train_data = sequences[:train_size]\n",
        "test_data = sequences[train_size:]\n",
        "\n",
        "# Split features and labels\n",
        "X_train, y_train = train_data[:, :-1, :], train_data[:, -1, 1]\n",
        "X_test, y_test = test_data[:, :-1, :], test_data[:, -1, 1]\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model with progress updates\n",
        "model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
        "\n",
        "# Make predictions for the next 1 year\n",
        "future_predictions = []\n",
        "\n",
        "# Use the last sequence from the test data\n",
        "current_sequence = X_test[-1]\n",
        "\n",
        "for _ in range(FUTURE_STEPS):\n",
        "    # Reshape the current sequence for prediction\n",
        "    current_sequence_reshaped = current_sequence.reshape((1, SEQUENCE_LENGTH - 1, 2))\n",
        "\n",
        "    # Predict the next sales value\n",
        "    next_prediction = model.predict(current_sequence_reshaped)\n",
        "\n",
        "    # Append the prediction to the list of future predictions\n",
        "    future_predictions.append(next_prediction[0, 0])\n",
        "\n",
        "    # Update the current sequence for the next iteration\n",
        "    current_sequence = np.append(current_sequence[1:], [[next_prediction[0, 0], 0]], axis=0)\n",
        "\n",
        "# Invert the scaling for better evaluation\n",
        "scaled_future_predictions = np.column_stack((np.zeros_like(future_predictions), future_predictions))\n",
        "scaled_future_predictions = scaler.inverse_transform(scaled_future_predictions)[:, 1]\n",
        "\n",
        "# Display the predictions for the next 1 year\n",
        "future_dates = pd.date_range(start=df['Date'].max() + pd.DateOffset(days=1), periods=FUTURE_STEPS, freq='D')\n",
        "future_predictions_df = pd.DataFrame({'Date': future_dates, 'Predicted_Sales': scaled_future_predictions})\n",
        "print(future_predictions_df)\n"
      ],
      "metadata": {
        "id": "tMcpfTLxlMWz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a9a263f4-2d39-4755-a224-8c8a746ac9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'create_sequences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-de481ea38869>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Create sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Split into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_sequences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.***Gaussian Processes***"
      ],
      "metadata": {
        "id": "qHms7vDtDIaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the kernel for the Gaussian Process\n",
        "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
        "\n",
        "# Create the Gaussian Process Regressor\n",
        "gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "gp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions, sigma = gp_model.predict(next_year_dates_scaled, return_std=True)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions, 'PredictionStdDev': sigma})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('gaussian_process_predictions_df.csv',index=False)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "files.download('gaussian_process_predictions_df.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "Mm2NMxmdCN0x",
        "outputId": "716274fa-d41d-4b37-8270-e8eb8eebe1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:430: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 0.01. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next year sales predictions:\n",
            "          Date  PredictedSales  PredictionStdDev\n",
            "0   2023-12-31      302.750000          0.000005\n",
            "1   2024-01-01      329.125000          0.000007\n",
            "2   2024-01-02      317.875000          0.000005\n",
            "3   2024-01-03      268.406250          0.000005\n",
            "4   2024-01-04      268.250000          0.000005\n",
            "..         ...             ...               ...\n",
            "360 2024-12-25      355.125000          0.000004\n",
            "361 2024-12-26      340.750000          0.000005\n",
            "362 2024-12-27      348.250000          0.000006\n",
            "363 2024-12-28      309.000000          0.000006\n",
            "364 2024-12-29      269.796875          0.000004\n",
            "\n",
            "[365 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a75cd801-5a10-40ff-8286-7115f7dc4fad\", \"gaussian_process_predictions_df.csv\", 14970)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.**Neural networks***"
      ],
      "metadata": {
        "id": "2PY9v18VEAeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(input())\n",
        "\n",
        "# Convert 'Date' to numerical representation\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['NumericDate'] = df['Date'].dt.dayofyear  # You can choose a different representation based on your needs\n",
        "\n",
        "# Assuming you have NumericDate and Sales columns\n",
        "X = df['NumericDate'].values.reshape(-1, 1)\n",
        "y = df['Sales'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build a simple neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=1))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))  # Linear activation for regression\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
        "\n",
        "# Generate dates for the next year\n",
        "next_year_dates = pd.date_range(df['Date'].max(), periods=365, freq='D')\n",
        "next_year_numeric_dates = next_year_dates.dayofyear.values.reshape(-1, 1)\n",
        "\n",
        "# Standardize next year's dates\n",
        "next_year_dates_scaled = scaler.transform(next_year_numeric_dates)\n",
        "\n",
        "# Make predictions\n",
        "sales_predictions = model.predict(next_year_dates_scaled)\n",
        "\n",
        "# Create a DataFrame with dates and predictions\n",
        "predictions_df = pd.DataFrame({'Date': next_year_dates, 'PredictedSales': sales_predictions.flatten()})\n",
        "\n",
        "# Display predictions\n",
        "print(\"Next year sales predictions:\")\n",
        "print(predictions_df)\n",
        "\n",
        "predictions_df.to_csv('neural_networks_predictions_df.csv',index=False)\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "files.download('neural_networks_predictions_df.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A_E3HqrRDQY7",
        "outputId": "5bb1720a-f6e9-41c8-e71a-55bc564b74c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Epoch 1/50\n",
            "46/46 [==============================] - 1s 6ms/step - loss: 94590.2109 - val_loss: 90318.8828\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 93274.8359 - val_loss: 88003.9375\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 89088.6797 - val_loss: 81521.1172\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 79533.1016 - val_loss: 68876.9766\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 63631.1523 - val_loss: 50578.6289\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 43383.9180 - val_loss: 30457.5664\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 24062.9160 - val_loss: 14538.8877\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 11145.9326 - val_loss: 6789.7227\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 5871.1689 - val_loss: 4742.0107\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 4531.0205 - val_loss: 4362.4980\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 4151.5972 - val_loss: 4157.8872\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3943.3374 - val_loss: 3992.9758\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3788.2773 - val_loss: 3866.8081\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3668.3723 - val_loss: 3746.1646\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3572.9797 - val_loss: 3667.9993\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 3502.7737 - val_loss: 3601.7888\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3447.2117 - val_loss: 3555.1736\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3400.7449 - val_loss: 3511.2258\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 3368.2705 - val_loss: 3481.2397\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 3345.2927 - val_loss: 3469.0691\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3325.6294 - val_loss: 3453.8679\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 3302.5698 - val_loss: 3423.3459\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3290.2480 - val_loss: 3423.3606\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3278.6392 - val_loss: 3410.9348\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3269.6572 - val_loss: 3403.4221\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3263.9678 - val_loss: 3416.8877\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3256.0352 - val_loss: 3388.7991\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3256.8674 - val_loss: 3403.0554\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3251.5615 - val_loss: 3388.4692\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3251.1438 - val_loss: 3373.0435\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3241.9929 - val_loss: 3386.5361\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 3244.5215 - val_loss: 3368.4561\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3242.8589 - val_loss: 3384.4666\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3237.6633 - val_loss: 3366.0293\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3241.6370 - val_loss: 3364.4666\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3234.3433 - val_loss: 3377.8547\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3234.4380 - val_loss: 3362.8330\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3232.7686 - val_loss: 3377.5969\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3232.8706 - val_loss: 3377.1780\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3230.6265 - val_loss: 3360.5310\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3231.1924 - val_loss: 3372.5276\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3233.5388 - val_loss: 3347.9375\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3229.8501 - val_loss: 3367.2722\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3229.3291 - val_loss: 3371.3311\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3230.1870 - val_loss: 3350.0532\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3233.4648 - val_loss: 3368.1028\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3230.0542 - val_loss: 3371.5874\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 3232.0383 - val_loss: 3364.4084\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3229.7781 - val_loss: 3400.6411\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 3232.2888 - val_loss: 3374.1985\n",
            "12/12 [==============================] - 0s 2ms/step\n",
            "Next year sales predictions:\n",
            "          Date  PredictedSales\n",
            "0   2023-12-31      314.204956\n",
            "1   2024-01-01      311.874573\n",
            "2   2024-01-02      311.335907\n",
            "3   2024-01-03      311.016418\n",
            "4   2024-01-04      310.743011\n",
            "..         ...             ...\n",
            "360 2024-12-25      311.190399\n",
            "361 2024-12-26      311.786804\n",
            "362 2024-12-27      312.383270\n",
            "363 2024-12-28      312.979706\n",
            "364 2024-12-29      313.576141\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_534f73f1-4efe-4c56-b956-3b0b4dc38b33\", \"neural_networks_predictions_df.csv\", 7563)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.**ARIMA**"
      ],
      "metadata": {
        "id": "pL5C4fsGS08Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from datetime import timedelta\n",
        "\n",
        "# Load your data from a CSV file or another source\n",
        "# Example assuming you have a CSV file named 'sales_data.csv'\n",
        "# df = pd.read_csv('sales_data.csv')\n",
        "\n",
        "# If you have the data in a DataFrame already, you can skip the data loading part\n",
        "df = pd.read_csv(input())\n",
        "# Set the 'Date' column as the index\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Fit an ARIMA model to the sales data\n",
        "model = ARIMA(df['Sales'], order=(5, 1, 2))  # Adjust order as needed\n",
        "results = model.fit()\n",
        "\n",
        "# Forecast sales for the next month (30 days)\n",
        "next_month_forecast = results.get_forecast(steps=365)\n",
        "next_month_index = pd.date_range(start=df.index[-1] + timedelta(days=1), periods=365)\n",
        "next_month_sales = next_month_forecast.predicted_mean.values\n",
        "\n",
        "# Round off the sales\n",
        "rounded_sales = pd.Series(next_month_sales).round().astype(int)\n",
        "\n",
        "# Create a DataFrame with rounded sales\n",
        "rounded_sales_df = pd.DataFrame({'Date': next_month_index, 'Sales': rounded_sales})\n",
        "\n",
        "# Save the rounded sales to a CSV file\n",
        "rounded_sales_df.to_csv('sales_predictions.csv', index=False)\n",
        "\n",
        "# Download the CSV file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('sales_predictions.csv')\n"
      ],
      "metadata": {
        "id": "OPjfUNqOQ6pz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "7b61a408-8fc2-472e-dfbd-525327b6b7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3d2c26ab-ede3-450b-bc27-147af72aca9e\", \"sales_predictions.csv\", 5486)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.**sarima**"
      ],
      "metadata": {
        "id": "6CrQyUGUU-is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from datetime import timedelta\n",
        "\n",
        "# Load your data from a CSV file or another source\n",
        "# Example assuming you have a CSV file named 'sales_data.csv'\n",
        "# df = pd.read_csv('sales_data.csv')\n",
        "\n",
        "# If you have the data in a DataFrame already, you can skip the data loading part\n",
        "df = pd.read_csv(input())\n",
        "# Set the 'Date' column as the index\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Specify the order for the SARIMA model\n",
        "order = (1, 1, 1)  # Non-seasonal order (p, d, q)\n",
        "seasonal_order = (1, 1, 1, 12)  # Seasonal order (P, D, Q, s)\n",
        "\n",
        "# Fit a SARIMA model to the sales data\n",
        "model_sarima = SARIMAX(df['Sales'], order=order, seasonal_order=seasonal_order)\n",
        "results_sarima = model_sarima.fit(disp=False)\n",
        "\n",
        "# Forecast sales for the next month (30 days)\n",
        "next_month_forecast = results_sarima.get_forecast(steps=365)\n",
        "next_month_index = pd.date_range(start=df.index[-1] + timedelta(days=1), periods=365)\n",
        "next_month_sales = next_month_forecast.predicted_mean\n",
        "\n",
        "# Round off the sales\n",
        "rounded_sales = pd.Series(next_month_sales).round().astype(int)\n",
        "\n",
        "# Create a DataFrame with rounded sales\n",
        "rounded_sales_df = pd.DataFrame({'Date': next_month_index, 'Sales': rounded_sales})\n",
        "\n",
        "# Save the rounded sales to a CSV file\n",
        "rounded_sales_df.to_csv('sales_predictions_sarima.csv', index=False)\n",
        "\n",
        "# Download the CSV file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('sales_predictions_sarima.csv')\n"
      ],
      "metadata": {
        "id": "txt0Y3DZQ6jA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "5ad67db6-fa1f-47ab-9402-2883d94be03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a065b45-f61c-4382-afb9-58796cb880f2\", \"sales_predictions_sarima.csv\", 5486)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.**LINEAR REGRESSION**"
      ],
      "metadata": {
        "id": "C6h9-tH9VxmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from datetime import timedelta\n",
        "\n",
        "# Load your data from a CSV file or another source\n",
        "# Example assuming you have a CSV file named 'sales_data.csv'\n",
        "# df = pd.read_csv('sales_data.csv')\n",
        "\n",
        "# If you have the data in a DataFrame already, you can skip the data loading part\n",
        "df = pd.read_csv(input())\n",
        "# Set the 'Date' column as the index\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Prepare the data for linear regression\n",
        "df['Day'] = (df.index - df.index[0]).days  # Add a column for the number of days since the start\n",
        "X = df[['Day']]\n",
        "y = df['Sales']\n",
        "\n",
        "# Fit a linear regression model\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(X, y)\n",
        "\n",
        "# Forecast sales for the next month (30 days)\n",
        "last_date = df.index[-1]\n",
        "next_month_days = pd.date_range(start=last_date + timedelta(days=1), periods=365, freq='D')\n",
        "next_month_day_numbers = (next_month_days - df.index[0]).days.values.reshape(-1, 1)\n",
        "next_month_forecast = model_lr.predict(next_month_day_numbers)\n",
        "\n",
        "# Round off the sales\n",
        "rounded_sales = pd.Series(next_month_forecast).round().astype(int)\n",
        "\n",
        "# Create a DataFrame with rounded sales\n",
        "next_month_forecast_df = pd.DataFrame({'Date': next_month_days, 'Sales': rounded_sales})\n",
        "\n",
        "# Save the rounded sales to a CSV file\n",
        "next_month_forecast_df.to_csv('sales_predictions_linear_regression.csv', index=False)\n",
        "\n",
        "# Download the CSV file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('sales_predictions_linear_regression.csv')\n"
      ],
      "metadata": {
        "id": "p_Bq3DUR6hSl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "ab1f6c5c-878f-4489-8f04-8226ee0eb317"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8c6931f2-bb6d-47b3-97c5-7a4e4a6e5401\", \"sales_predictions_linear_regression.csv\", 5486)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.RANDOM FOREST"
      ],
      "metadata": {
        "id": "O5-hJMRBWK8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load your sales data CSV file\n",
        "file_path = input()\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Assuming your CSV file has \"Date\" and \"Sales\" columns\n",
        "# Convert the \"Date\" column to a datetime object\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Extract features (you may need to engineer more features depending on your data)\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "# Define features and target variable\n",
        "features = ['Month', 'Day', 'DayOfWeek']\n",
        "target = 'Sales'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(train_df[features], train_df[target])\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = rf_model.predict(test_df[features])\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "rmse = sqrt(mean_squared_error(test_df[target], predictions))\n",
        "print(f\"Root Mean Squared Error on Test Set: {rmse}\")\n",
        "\n",
        "# Now, let's make predictions for the next year without a for loop\n",
        "# Generate future dates\n",
        "future_dates = pd.date_range(df['Date'].max() + pd.DateOffset(1), periods=365, freq='D')\n",
        "\n",
        "# Create a DataFrame for future dates with the same additional features\n",
        "future_df = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Month': future_dates.month,\n",
        "    'Day': future_dates.day,\n",
        "    'DayOfWeek': future_dates.dayofweek\n",
        "})\n",
        "\n",
        "# Make predictions for the next year\n",
        "future_predictions = rf_model.predict(future_df[features])\n",
        "\n",
        "# Add the predictions to the future DataFrame\n",
        "future_df['PredictedSales'] = future_predictions\n",
        "\n",
        "# Print the predictions for the next year\n",
        "print(future_df[['Date', 'PredictedSales']])\n",
        "\n",
        "\n",
        "future_df.to_csv(\"randomforest.csv\",index=False)\n",
        "files.download(\"randomforest.csv\")"
      ],
      "metadata": {
        "id": "vttQhzHl6hLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "7a9024c4-5b19-4700-ead3-c2d279f1cd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "Root Mean Squared Error on Test Set: 62.63961283981018\n",
            "          Date  PredictedSales\n",
            "0   2024-01-01          351.30\n",
            "1   2024-01-02          287.11\n",
            "2   2024-01-03          264.99\n",
            "3   2024-01-04          278.18\n",
            "4   2024-01-05          281.21\n",
            "..         ...             ...\n",
            "360 2024-12-26          298.45\n",
            "361 2024-12-27          347.19\n",
            "362 2024-12-28          356.77\n",
            "363 2024-12-29          318.51\n",
            "364 2024-12-30          300.70\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_54782c42-e893-4ee6-a249-b3faf3f0d9cd\", \"randomforest.csv\", 9111)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.**EXPONENTIAL SMOOTHNING**"
      ],
      "metadata": {
        "id": "DtCXbY1yYZfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "# Load your sales data from the CSV file\n",
        "df = pd.read_csv(input())\n",
        "\n",
        "# Assuming 'Date' is in datetime format, if not, convert it\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# Set 'Date' as the index\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Perform exponential smoothing\n",
        "model = ExponentialSmoothing(df['Sales'], seasonal='add', seasonal_periods=12)\n",
        "result = model.fit()\n",
        "\n",
        "# Predict next year sales\n",
        "next_year_start = df.index[-1] + pd.DateOffset(days=1)\n",
        "next_year_end = next_year_start + pd.DateOffset(years=1)\n",
        "forecast = result.predict(start=next_year_start, end=next_year_end)\n",
        "\n",
        "# Print the predicted sales for the next year\n",
        "print(forecast)\n",
        "forecast.to_csv(\"expo_smothning.csv\",index=False)\n",
        "\n",
        "files.download(\"expo_smothning.csv\")"
      ],
      "metadata": {
        "id": "r1sowyQGOVnL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "bc4bae5b-a299-4217-8a4d-6a4a3a7d65b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/e/2PACX-1vSR9AkygTVlvkkeQrYdWZoIr7M2OW4WALjERGSBYQmkMaXG4-EG8BGtQ5Ihgb5RvtyY9EEKlp3H5JAL/pub?output=csv\n",
            "2024-01-01    297.547095\n",
            "2024-01-02    318.337414\n",
            "2024-01-03    314.314105\n",
            "2024-01-04    295.570677\n",
            "2024-01-05    315.986000\n",
            "                 ...    \n",
            "2024-12-28    314.314105\n",
            "2024-12-29    295.570677\n",
            "2024-12-30    315.986000\n",
            "2024-12-31    295.169492\n",
            "2025-01-01    298.131908\n",
            "Freq: D, Length: 367, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
            "  self._init_dates(dates, freq)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a0155558-c9d8-492f-8230-b09da018b52c\", \"expo_smothning.csv\", 6761)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PqsOU2DJOVh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.***XG BOOST***"
      ],
      "metadata": {
        "id": "0G1kB_ysdXMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X_train = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y_train = train_data['Sales']\n",
        "\n",
        "# Initialize and train the XGBoost model\n",
        "model = XGBRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "next_year_data.to_csv('xgboostnext_year_data.csv',index=False)\n",
        "from google.colab import files\n",
        "files.download('xgboostnext_year_data.csv')\n"
      ],
      "metadata": {
        "id": "8esZNMxvgU-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "56f232fd-ce2c-4c97-d36a-ab2afe99861b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date  predicted_sales\n",
            "0   2023-01-01       316.858734\n",
            "1   2023-01-02       321.341125\n",
            "2   2023-01-03       293.347870\n",
            "3   2023-01-04       248.854767\n",
            "4   2023-01-05       294.287201\n",
            "..         ...              ...\n",
            "360 2023-12-27       330.177582\n",
            "361 2023-12-28       366.851227\n",
            "362 2023-12-29       360.884705\n",
            "363 2023-12-30       312.343567\n",
            "364 2023-12-31       295.378754\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f3e82903-7adf-4439-8202-6af18db275a7\", \"xgboostnext_year_data.csv\", 14381)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.***ELASTIC NET REG***"
      ],
      "metadata": {
        "id": "5NNnTFHjdzhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X_train = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y_train = train_data['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Elastic Net model\n",
        "alpha = 0.5  # L1 regularization term\n",
        "l1_ratio = 0.5  # Elastic Net mixing parameter\n",
        "model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using RMSE (Root Mean Squared Error)\n",
        "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
        "print(f'Root Mean Squared Error on test set: {rmse}')\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "\n",
        "next_year_data.to_csv('elasticnetnext_year_data.csv',index=False)\n",
        "\n",
        "\n",
        "files.download('elasticnetnext_year_data.csv')"
      ],
      "metadata": {
        "id": "oX73zAvmgUz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "f1385b26-7bf1-47f3-85d8-164f9869cdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error on test set: 63.85815037314253\n",
            "          Date  predicted_sales\n",
            "0   2023-01-01       337.216572\n",
            "1   2023-01-02       302.903838\n",
            "2   2023-01-03       301.217343\n",
            "3   2023-01-04       299.530847\n",
            "4   2023-01-05       297.844352\n",
            "..         ...              ...\n",
            "360 2023-12-27       288.951588\n",
            "361 2023-12-28       287.265093\n",
            "362 2023-12-29       285.578597\n",
            "363 2023-12-30       283.892102\n",
            "364 2023-12-31       282.205606\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_32f1ed1f-a924-4cb6-93e5-f2b8df075eb3\", \"elasticnetnext_year_data.csv\", 17544)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.***LIGHTGBM REG***"
      ],
      "metadata": {
        "id": "JDnzYab_eO_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X_train = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y_train = train_data['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the LightGBM model\n",
        "model = LGBMRegressor()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using RMSE (Root Mean Squared Error)\n",
        "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
        "print(f'Root Mean Squared Error on test set: {rmse}')\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "\n",
        "next_year_data.to_csv('LIGHTGBMnext_year_data.csv',index=False)\n",
        "\n",
        "files.download('LIGHTGBMnext_year_data.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QGreUikfgUo-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13f34cf8-e2fc-4dc1-dd7b-a5d2ed2e95d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000057 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 202\n",
            "[LightGBM] [Info] Number of data points in the train set: 292, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 292.794521\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Root Mean Squared Error on test set: 66.87293193868021\n",
            "          Date  predicted_sales\n",
            "0   2023-01-01       313.612752\n",
            "1   2023-01-02       331.781118\n",
            "2   2023-01-03       322.503138\n",
            "3   2023-01-04       297.650847\n",
            "4   2023-01-05       281.346245\n",
            "..         ...              ...\n",
            "360 2023-12-27       308.883510\n",
            "361 2023-12-28       313.201072\n",
            "362 2023-12-29       310.857699\n",
            "363 2023-12-30       312.882131\n",
            "364 2023-12-31       320.598318\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_08a69a36-b5ee-495e-8c63-cfc64cc99196\", \"LIGHTGBMnext_year_data.csv\", 17578)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.***CATBOOST***"
      ],
      "metadata": {
        "id": "fHHo1D7Mew7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "id": "6KJ2NTOuelnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X_train = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y_train = train_data['Sales']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost model\n",
        "model = CatBoostRegressor()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using RMSE (Root Mean Squared Error)\n",
        "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
        "print(f'Root Mean Squared Error on test set: {rmse}')\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "\n",
        "next_year_data.to_csv('catboostnext_year_data.csv',index=False)\n",
        "\n",
        "files.download('catboostnext_year_data.csv')"
      ],
      "metadata": {
        "id": "1rvQz49OimBp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ddc90f07-2ed9-4e11-eccc-b5d386ade69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate set to 0.033706\n",
            "0:\tlearn: 56.8604405\ttotal: 1.87ms\tremaining: 1.87s\n",
            "1:\tlearn: 56.6673915\ttotal: 4.15ms\tremaining: 2.07s\n",
            "2:\tlearn: 56.5096801\ttotal: 6.11ms\tremaining: 2.03s\n",
            "3:\tlearn: 56.3547623\ttotal: 7.97ms\tremaining: 1.98s\n",
            "4:\tlearn: 56.2345122\ttotal: 9.72ms\tremaining: 1.93s\n",
            "5:\tlearn: 56.0963271\ttotal: 11.3ms\tremaining: 1.88s\n",
            "6:\tlearn: 55.9699626\ttotal: 12.9ms\tremaining: 1.83s\n",
            "7:\tlearn: 55.8136126\ttotal: 14.3ms\tremaining: 1.78s\n",
            "8:\tlearn: 55.7009097\ttotal: 15.9ms\tremaining: 1.75s\n",
            "9:\tlearn: 55.5583537\ttotal: 17.6ms\tremaining: 1.74s\n",
            "10:\tlearn: 55.4374952\ttotal: 19.4ms\tremaining: 1.75s\n",
            "11:\tlearn: 55.3298284\ttotal: 21.2ms\tremaining: 1.74s\n",
            "12:\tlearn: 55.1977398\ttotal: 22.8ms\tremaining: 1.73s\n",
            "13:\tlearn: 55.0832768\ttotal: 24.5ms\tremaining: 1.72s\n",
            "14:\tlearn: 54.9543547\ttotal: 26.1ms\tremaining: 1.71s\n",
            "15:\tlearn: 54.7995836\ttotal: 32.6ms\tremaining: 2s\n",
            "16:\tlearn: 54.6548291\ttotal: 33.3ms\tremaining: 1.93s\n",
            "17:\tlearn: 54.5451051\ttotal: 37.8ms\tremaining: 2.06s\n",
            "18:\tlearn: 54.4736815\ttotal: 39.7ms\tremaining: 2.05s\n",
            "19:\tlearn: 54.3713913\ttotal: 42.6ms\tremaining: 2.09s\n",
            "20:\tlearn: 54.2902002\ttotal: 48.8ms\tremaining: 2.28s\n",
            "21:\tlearn: 54.2117562\ttotal: 50.1ms\tremaining: 2.23s\n",
            "22:\tlearn: 54.0895067\ttotal: 51.2ms\tremaining: 2.18s\n",
            "23:\tlearn: 54.0220249\ttotal: 52.6ms\tremaining: 2.14s\n",
            "24:\tlearn: 53.8705537\ttotal: 54.3ms\tremaining: 2.12s\n",
            "25:\tlearn: 53.7608124\ttotal: 56.1ms\tremaining: 2.1s\n",
            "26:\tlearn: 53.6414785\ttotal: 63.3ms\tremaining: 2.28s\n",
            "27:\tlearn: 53.5518005\ttotal: 64.5ms\tremaining: 2.24s\n",
            "28:\tlearn: 53.4431730\ttotal: 65.5ms\tremaining: 2.19s\n",
            "29:\tlearn: 53.3698241\ttotal: 66.6ms\tremaining: 2.15s\n",
            "30:\tlearn: 53.2619939\ttotal: 68.1ms\tremaining: 2.13s\n",
            "31:\tlearn: 53.1695180\ttotal: 69.8ms\tremaining: 2.11s\n",
            "32:\tlearn: 53.0714140\ttotal: 76.3ms\tremaining: 2.24s\n",
            "33:\tlearn: 52.9587375\ttotal: 77.5ms\tremaining: 2.2s\n",
            "34:\tlearn: 52.8839660\ttotal: 78.5ms\tremaining: 2.17s\n",
            "35:\tlearn: 52.7450609\ttotal: 86ms\tremaining: 2.3s\n",
            "36:\tlearn: 52.6185548\ttotal: 89.6ms\tremaining: 2.33s\n",
            "37:\tlearn: 52.5658757\ttotal: 96.3ms\tremaining: 2.44s\n",
            "38:\tlearn: 52.5025719\ttotal: 97.5ms\tremaining: 2.4s\n",
            "39:\tlearn: 52.4399247\ttotal: 98.6ms\tremaining: 2.37s\n",
            "40:\tlearn: 52.3345907\ttotal: 99.7ms\tremaining: 2.33s\n",
            "41:\tlearn: 52.2506796\ttotal: 101ms\tremaining: 2.31s\n",
            "42:\tlearn: 52.1990559\ttotal: 103ms\tremaining: 2.3s\n",
            "43:\tlearn: 52.1066615\ttotal: 108ms\tremaining: 2.35s\n",
            "44:\tlearn: 52.0372483\ttotal: 115ms\tremaining: 2.44s\n",
            "45:\tlearn: 51.9473593\ttotal: 116ms\tremaining: 2.41s\n",
            "46:\tlearn: 51.8605457\ttotal: 117ms\tremaining: 2.38s\n",
            "47:\tlearn: 51.7794471\ttotal: 119ms\tremaining: 2.36s\n",
            "48:\tlearn: 51.7075979\ttotal: 121ms\tremaining: 2.35s\n",
            "49:\tlearn: 51.6564167\ttotal: 123ms\tremaining: 2.33s\n",
            "50:\tlearn: 51.5528510\ttotal: 130ms\tremaining: 2.42s\n",
            "51:\tlearn: 51.4885877\ttotal: 131ms\tremaining: 2.39s\n",
            "52:\tlearn: 51.3804488\ttotal: 132ms\tremaining: 2.37s\n",
            "53:\tlearn: 51.2864118\ttotal: 134ms\tremaining: 2.34s\n",
            "54:\tlearn: 51.2275271\ttotal: 135ms\tremaining: 2.33s\n",
            "55:\tlearn: 51.1641523\ttotal: 138ms\tremaining: 2.32s\n",
            "56:\tlearn: 51.0792822\ttotal: 139ms\tremaining: 2.29s\n",
            "57:\tlearn: 50.9895393\ttotal: 140ms\tremaining: 2.28s\n",
            "58:\tlearn: 50.8979292\ttotal: 142ms\tremaining: 2.27s\n",
            "59:\tlearn: 50.8258875\ttotal: 149ms\tremaining: 2.33s\n",
            "60:\tlearn: 50.7489246\ttotal: 151ms\tremaining: 2.33s\n",
            "61:\tlearn: 50.6661911\ttotal: 153ms\tremaining: 2.31s\n",
            "62:\tlearn: 50.6211567\ttotal: 154ms\tremaining: 2.28s\n",
            "63:\tlearn: 50.5230534\ttotal: 160ms\tremaining: 2.35s\n",
            "64:\tlearn: 50.4394504\ttotal: 161ms\tremaining: 2.32s\n",
            "65:\tlearn: 50.3861926\ttotal: 169ms\tremaining: 2.39s\n",
            "66:\tlearn: 50.2795581\ttotal: 170ms\tremaining: 2.37s\n",
            "67:\tlearn: 50.2252313\ttotal: 172ms\tremaining: 2.35s\n",
            "68:\tlearn: 50.1788123\ttotal: 173ms\tremaining: 2.33s\n",
            "69:\tlearn: 50.1145670\ttotal: 174ms\tremaining: 2.32s\n",
            "70:\tlearn: 50.0337577\ttotal: 176ms\tremaining: 2.31s\n",
            "71:\tlearn: 50.0139263\ttotal: 183ms\tremaining: 2.35s\n",
            "72:\tlearn: 49.9542627\ttotal: 184ms\tremaining: 2.33s\n",
            "73:\tlearn: 49.9316421\ttotal: 185ms\tremaining: 2.32s\n",
            "74:\tlearn: 49.8875331\ttotal: 186ms\tremaining: 2.3s\n",
            "75:\tlearn: 49.8418803\ttotal: 189ms\tremaining: 2.29s\n",
            "76:\tlearn: 49.7586622\ttotal: 191ms\tremaining: 2.28s\n",
            "77:\tlearn: 49.7305589\ttotal: 191ms\tremaining: 2.26s\n",
            "78:\tlearn: 49.6563493\ttotal: 200ms\tremaining: 2.33s\n",
            "79:\tlearn: 49.5484028\ttotal: 201ms\tremaining: 2.31s\n",
            "80:\tlearn: 49.4820625\ttotal: 202ms\tremaining: 2.29s\n",
            "81:\tlearn: 49.4226645\ttotal: 203ms\tremaining: 2.28s\n",
            "82:\tlearn: 49.3278911\ttotal: 205ms\tremaining: 2.27s\n",
            "83:\tlearn: 49.2586701\ttotal: 211ms\tremaining: 2.3s\n",
            "84:\tlearn: 49.2301616\ttotal: 218ms\tremaining: 2.34s\n",
            "85:\tlearn: 49.1783537\ttotal: 222ms\tremaining: 2.36s\n",
            "86:\tlearn: 49.1101713\ttotal: 228ms\tremaining: 2.39s\n",
            "87:\tlearn: 49.0404643\ttotal: 230ms\tremaining: 2.39s\n",
            "88:\tlearn: 49.0080612\ttotal: 236ms\tremaining: 2.42s\n",
            "89:\tlearn: 48.9476897\ttotal: 244ms\tremaining: 2.47s\n",
            "90:\tlearn: 48.8733221\ttotal: 248ms\tremaining: 2.47s\n",
            "91:\tlearn: 48.8026203\ttotal: 250ms\tremaining: 2.46s\n",
            "92:\tlearn: 48.7433529\ttotal: 253ms\tremaining: 2.46s\n",
            "93:\tlearn: 48.6626020\ttotal: 255ms\tremaining: 2.45s\n",
            "94:\tlearn: 48.6055957\ttotal: 262ms\tremaining: 2.49s\n",
            "95:\tlearn: 48.5463798\ttotal: 274ms\tremaining: 2.58s\n",
            "96:\tlearn: 48.5165676\ttotal: 280ms\tremaining: 2.6s\n",
            "97:\tlearn: 48.4757661\ttotal: 285ms\tremaining: 2.62s\n",
            "98:\tlearn: 48.4579293\ttotal: 291ms\tremaining: 2.65s\n",
            "99:\tlearn: 48.3829634\ttotal: 300ms\tremaining: 2.7s\n",
            "100:\tlearn: 48.3505909\ttotal: 301ms\tremaining: 2.68s\n",
            "101:\tlearn: 48.2907682\ttotal: 302ms\tremaining: 2.66s\n",
            "102:\tlearn: 48.2070232\ttotal: 303ms\tremaining: 2.64s\n",
            "103:\tlearn: 48.1772511\ttotal: 304ms\tremaining: 2.62s\n",
            "104:\tlearn: 48.0825899\ttotal: 306ms\tremaining: 2.61s\n",
            "105:\tlearn: 47.9862168\ttotal: 308ms\tremaining: 2.6s\n",
            "106:\tlearn: 47.9576540\ttotal: 314ms\tremaining: 2.62s\n",
            "107:\tlearn: 47.9010530\ttotal: 315ms\tremaining: 2.6s\n",
            "108:\tlearn: 47.8538611\ttotal: 316ms\tremaining: 2.58s\n",
            "109:\tlearn: 47.8067902\ttotal: 317ms\tremaining: 2.56s\n",
            "110:\tlearn: 47.7610185\ttotal: 322ms\tremaining: 2.58s\n",
            "111:\tlearn: 47.6855871\ttotal: 331ms\tremaining: 2.62s\n",
            "112:\tlearn: 47.6611306\ttotal: 338ms\tremaining: 2.65s\n",
            "113:\tlearn: 47.6119765\ttotal: 339ms\tremaining: 2.64s\n",
            "114:\tlearn: 47.5515310\ttotal: 340ms\tremaining: 2.62s\n",
            "115:\tlearn: 47.5068375\ttotal: 341ms\tremaining: 2.6s\n",
            "116:\tlearn: 47.4603930\ttotal: 346ms\tremaining: 2.61s\n",
            "117:\tlearn: 47.3937428\ttotal: 348ms\tremaining: 2.6s\n",
            "118:\tlearn: 47.3303800\ttotal: 350ms\tremaining: 2.59s\n",
            "119:\tlearn: 47.2536326\ttotal: 356ms\tremaining: 2.61s\n",
            "120:\tlearn: 47.2272449\ttotal: 358ms\tremaining: 2.6s\n",
            "121:\tlearn: 47.1581521\ttotal: 359ms\tremaining: 2.58s\n",
            "122:\tlearn: 47.1146206\ttotal: 360ms\tremaining: 2.57s\n",
            "123:\tlearn: 47.0674713\ttotal: 361ms\tremaining: 2.55s\n",
            "124:\tlearn: 47.0181743\ttotal: 363ms\tremaining: 2.54s\n",
            "125:\tlearn: 46.9651362\ttotal: 365ms\tremaining: 2.53s\n",
            "126:\tlearn: 46.8922215\ttotal: 371ms\tremaining: 2.55s\n",
            "127:\tlearn: 46.8692400\ttotal: 372ms\tremaining: 2.54s\n",
            "128:\tlearn: 46.8248280\ttotal: 374ms\tremaining: 2.52s\n",
            "129:\tlearn: 46.7685839\ttotal: 375ms\tremaining: 2.51s\n",
            "130:\tlearn: 46.7000514\ttotal: 377ms\tremaining: 2.5s\n",
            "131:\tlearn: 46.6675711\ttotal: 383ms\tremaining: 2.52s\n",
            "132:\tlearn: 46.6163741\ttotal: 385ms\tremaining: 2.51s\n",
            "133:\tlearn: 46.5681644\ttotal: 386ms\tremaining: 2.49s\n",
            "134:\tlearn: 46.5268093\ttotal: 388ms\tremaining: 2.48s\n",
            "135:\tlearn: 46.4639762\ttotal: 390ms\tremaining: 2.47s\n",
            "136:\tlearn: 46.4027927\ttotal: 397ms\tremaining: 2.5s\n",
            "137:\tlearn: 46.3775083\ttotal: 398ms\tremaining: 2.48s\n",
            "138:\tlearn: 46.3351735\ttotal: 399ms\tremaining: 2.47s\n",
            "139:\tlearn: 46.2571364\ttotal: 400ms\tremaining: 2.46s\n",
            "140:\tlearn: 46.1957004\ttotal: 402ms\tremaining: 2.45s\n",
            "141:\tlearn: 46.1334853\ttotal: 404ms\tremaining: 2.44s\n",
            "142:\tlearn: 46.0679591\ttotal: 408ms\tremaining: 2.44s\n",
            "143:\tlearn: 45.9827155\ttotal: 410ms\tremaining: 2.44s\n",
            "144:\tlearn: 45.9235076\ttotal: 414ms\tremaining: 2.44s\n",
            "145:\tlearn: 45.8708663\ttotal: 422ms\tremaining: 2.47s\n",
            "146:\tlearn: 45.8326565\ttotal: 423ms\tremaining: 2.45s\n",
            "147:\tlearn: 45.7871570\ttotal: 424ms\tremaining: 2.44s\n",
            "148:\tlearn: 45.7304037\ttotal: 426ms\tremaining: 2.43s\n",
            "149:\tlearn: 45.6416732\ttotal: 428ms\tremaining: 2.42s\n",
            "150:\tlearn: 45.5697262\ttotal: 434ms\tremaining: 2.44s\n",
            "151:\tlearn: 45.5388862\ttotal: 436ms\tremaining: 2.43s\n",
            "152:\tlearn: 45.4895254\ttotal: 437ms\tremaining: 2.42s\n",
            "153:\tlearn: 45.4550598\ttotal: 438ms\tremaining: 2.41s\n",
            "154:\tlearn: 45.4136301\ttotal: 440ms\tremaining: 2.4s\n",
            "155:\tlearn: 45.3721170\ttotal: 442ms\tremaining: 2.39s\n",
            "156:\tlearn: 45.3170742\ttotal: 447ms\tremaining: 2.4s\n",
            "157:\tlearn: 45.2931366\ttotal: 448ms\tremaining: 2.39s\n",
            "158:\tlearn: 45.2693267\ttotal: 454ms\tremaining: 2.4s\n",
            "159:\tlearn: 45.2509570\ttotal: 458ms\tremaining: 2.4s\n",
            "160:\tlearn: 45.2141438\ttotal: 458ms\tremaining: 2.39s\n",
            "161:\tlearn: 45.1705346\ttotal: 459ms\tremaining: 2.37s\n",
            "162:\tlearn: 45.0876547\ttotal: 460ms\tremaining: 2.36s\n",
            "163:\tlearn: 45.0003148\ttotal: 460ms\tremaining: 2.35s\n",
            "164:\tlearn: 44.9272289\ttotal: 462ms\tremaining: 2.33s\n",
            "165:\tlearn: 44.9100374\ttotal: 467ms\tremaining: 2.35s\n",
            "166:\tlearn: 44.8445284\ttotal: 467ms\tremaining: 2.33s\n",
            "167:\tlearn: 44.7989690\ttotal: 468ms\tremaining: 2.32s\n",
            "168:\tlearn: 44.7194422\ttotal: 469ms\tremaining: 2.3s\n",
            "169:\tlearn: 44.6515696\ttotal: 469ms\tremaining: 2.29s\n",
            "170:\tlearn: 44.6075470\ttotal: 470ms\tremaining: 2.28s\n",
            "171:\tlearn: 44.5296983\ttotal: 473ms\tremaining: 2.28s\n",
            "172:\tlearn: 44.4849129\ttotal: 474ms\tremaining: 2.26s\n",
            "173:\tlearn: 44.4592596\ttotal: 474ms\tremaining: 2.25s\n",
            "174:\tlearn: 44.4009086\ttotal: 480ms\tremaining: 2.26s\n",
            "175:\tlearn: 44.3614937\ttotal: 480ms\tremaining: 2.25s\n",
            "176:\tlearn: 44.2887647\ttotal: 481ms\tremaining: 2.24s\n",
            "177:\tlearn: 44.2228589\ttotal: 482ms\tremaining: 2.22s\n",
            "178:\tlearn: 44.1863524\ttotal: 482ms\tremaining: 2.21s\n",
            "179:\tlearn: 44.1321385\ttotal: 483ms\tremaining: 2.2s\n",
            "180:\tlearn: 44.0288100\ttotal: 488ms\tremaining: 2.21s\n",
            "181:\tlearn: 43.9731884\ttotal: 488ms\tremaining: 2.19s\n",
            "182:\tlearn: 43.9034329\ttotal: 489ms\tremaining: 2.18s\n",
            "183:\tlearn: 43.8768747\ttotal: 490ms\tremaining: 2.17s\n",
            "184:\tlearn: 43.8436755\ttotal: 490ms\tremaining: 2.16s\n",
            "185:\tlearn: 43.8181941\ttotal: 492ms\tremaining: 2.15s\n",
            "186:\tlearn: 43.7392703\ttotal: 498ms\tremaining: 2.16s\n",
            "187:\tlearn: 43.6936312\ttotal: 498ms\tremaining: 2.15s\n",
            "188:\tlearn: 43.6819038\ttotal: 499ms\tremaining: 2.14s\n",
            "189:\tlearn: 43.6372408\ttotal: 500ms\tremaining: 2.13s\n",
            "190:\tlearn: 43.6055215\ttotal: 500ms\tremaining: 2.12s\n",
            "191:\tlearn: 43.5484210\ttotal: 501ms\tremaining: 2.11s\n",
            "192:\tlearn: 43.4694942\ttotal: 502ms\tremaining: 2.1s\n",
            "193:\tlearn: 43.4167128\ttotal: 505ms\tremaining: 2.1s\n",
            "194:\tlearn: 43.3727799\ttotal: 506ms\tremaining: 2.09s\n",
            "195:\tlearn: 43.3200928\ttotal: 506ms\tremaining: 2.08s\n",
            "196:\tlearn: 43.2932660\ttotal: 511ms\tremaining: 2.08s\n",
            "197:\tlearn: 43.2059618\ttotal: 512ms\tremaining: 2.07s\n",
            "198:\tlearn: 43.1589681\ttotal: 513ms\tremaining: 2.06s\n",
            "199:\tlearn: 43.1472227\ttotal: 515ms\tremaining: 2.06s\n",
            "200:\tlearn: 43.0974131\ttotal: 515ms\tremaining: 2.05s\n",
            "201:\tlearn: 43.0083927\ttotal: 520ms\tremaining: 2.06s\n",
            "202:\tlearn: 42.9766370\ttotal: 521ms\tremaining: 2.05s\n",
            "203:\tlearn: 42.8975022\ttotal: 522ms\tremaining: 2.04s\n",
            "204:\tlearn: 42.8651040\ttotal: 522ms\tremaining: 2.02s\n",
            "205:\tlearn: 42.7992887\ttotal: 523ms\tremaining: 2.02s\n",
            "206:\tlearn: 42.7643805\ttotal: 528ms\tremaining: 2.02s\n",
            "207:\tlearn: 42.7315920\ttotal: 528ms\tremaining: 2.01s\n",
            "208:\tlearn: 42.6786228\ttotal: 529ms\tremaining: 2s\n",
            "209:\tlearn: 42.6466257\ttotal: 530ms\tremaining: 1.99s\n",
            "210:\tlearn: 42.6068851\ttotal: 530ms\tremaining: 1.98s\n",
            "211:\tlearn: 42.5682764\ttotal: 531ms\tremaining: 1.97s\n",
            "212:\tlearn: 42.5369156\ttotal: 532ms\tremaining: 1.96s\n",
            "213:\tlearn: 42.4846532\ttotal: 532ms\tremaining: 1.96s\n",
            "214:\tlearn: 42.4112934\ttotal: 533ms\tremaining: 1.95s\n",
            "215:\tlearn: 42.3739723\ttotal: 534ms\tremaining: 1.94s\n",
            "216:\tlearn: 42.3369226\ttotal: 534ms\tremaining: 1.93s\n",
            "217:\tlearn: 42.2909561\ttotal: 538ms\tremaining: 1.93s\n",
            "218:\tlearn: 42.2630861\ttotal: 539ms\tremaining: 1.92s\n",
            "219:\tlearn: 42.2193565\ttotal: 540ms\tremaining: 1.91s\n",
            "220:\tlearn: 42.1788313\ttotal: 540ms\tremaining: 1.9s\n",
            "221:\tlearn: 42.1487152\ttotal: 545ms\tremaining: 1.91s\n",
            "222:\tlearn: 42.0992727\ttotal: 546ms\tremaining: 1.9s\n",
            "223:\tlearn: 42.0801496\ttotal: 546ms\tremaining: 1.89s\n",
            "224:\tlearn: 42.0478178\ttotal: 547ms\tremaining: 1.88s\n",
            "225:\tlearn: 41.9987045\ttotal: 547ms\tremaining: 1.87s\n",
            "226:\tlearn: 41.9599587\ttotal: 548ms\tremaining: 1.86s\n",
            "227:\tlearn: 41.9018213\ttotal: 553ms\tremaining: 1.87s\n",
            "228:\tlearn: 41.8790478\ttotal: 554ms\tremaining: 1.86s\n",
            "229:\tlearn: 41.8241036\ttotal: 554ms\tremaining: 1.85s\n",
            "230:\tlearn: 41.7565129\ttotal: 555ms\tremaining: 1.85s\n",
            "231:\tlearn: 41.7208833\ttotal: 555ms\tremaining: 1.84s\n",
            "232:\tlearn: 41.6723735\ttotal: 561ms\tremaining: 1.85s\n",
            "233:\tlearn: 41.5977737\ttotal: 562ms\tremaining: 1.84s\n",
            "234:\tlearn: 41.5520245\ttotal: 563ms\tremaining: 1.83s\n",
            "235:\tlearn: 41.4978060\ttotal: 564ms\tremaining: 1.82s\n",
            "236:\tlearn: 41.4668607\ttotal: 564ms\tremaining: 1.82s\n",
            "237:\tlearn: 41.4423525\ttotal: 565ms\tremaining: 1.81s\n",
            "238:\tlearn: 41.4098447\ttotal: 569ms\tremaining: 1.81s\n",
            "239:\tlearn: 41.3553229\ttotal: 570ms\tremaining: 1.8s\n",
            "240:\tlearn: 41.3027997\ttotal: 570ms\tremaining: 1.79s\n",
            "241:\tlearn: 41.2490438\ttotal: 571ms\tremaining: 1.79s\n",
            "242:\tlearn: 41.2243508\ttotal: 576ms\tremaining: 1.79s\n",
            "243:\tlearn: 41.1627946\ttotal: 577ms\tremaining: 1.79s\n",
            "244:\tlearn: 41.0822889\ttotal: 578ms\tremaining: 1.78s\n",
            "245:\tlearn: 41.0515091\ttotal: 578ms\tremaining: 1.77s\n",
            "246:\tlearn: 40.9823426\ttotal: 579ms\tremaining: 1.76s\n",
            "247:\tlearn: 40.9695171\ttotal: 579ms\tremaining: 1.76s\n",
            "248:\tlearn: 40.9651481\ttotal: 587ms\tremaining: 1.77s\n",
            "249:\tlearn: 40.9344110\ttotal: 589ms\tremaining: 1.77s\n",
            "250:\tlearn: 40.8878115\ttotal: 596ms\tremaining: 1.78s\n",
            "251:\tlearn: 40.8523263\ttotal: 597ms\tremaining: 1.77s\n",
            "252:\tlearn: 40.8045931\ttotal: 598ms\tremaining: 1.76s\n",
            "253:\tlearn: 40.7856282\ttotal: 600ms\tremaining: 1.76s\n",
            "254:\tlearn: 40.7618899\ttotal: 602ms\tremaining: 1.76s\n",
            "255:\tlearn: 40.7366264\ttotal: 608ms\tremaining: 1.77s\n",
            "256:\tlearn: 40.6795380\ttotal: 610ms\tremaining: 1.76s\n",
            "257:\tlearn: 40.6108382\ttotal: 611ms\tremaining: 1.76s\n",
            "258:\tlearn: 40.5803579\ttotal: 612ms\tremaining: 1.75s\n",
            "259:\tlearn: 40.5572826\ttotal: 613ms\tremaining: 1.74s\n",
            "260:\tlearn: 40.5303499\ttotal: 614ms\tremaining: 1.74s\n",
            "261:\tlearn: 40.4838482\ttotal: 616ms\tremaining: 1.73s\n",
            "262:\tlearn: 40.4256099\ttotal: 618ms\tremaining: 1.73s\n",
            "263:\tlearn: 40.3678359\ttotal: 624ms\tremaining: 1.74s\n",
            "264:\tlearn: 40.3233727\ttotal: 626ms\tremaining: 1.73s\n",
            "265:\tlearn: 40.2929904\ttotal: 627ms\tremaining: 1.73s\n",
            "266:\tlearn: 40.2669781\ttotal: 628ms\tremaining: 1.72s\n",
            "267:\tlearn: 40.2112424\ttotal: 630ms\tremaining: 1.72s\n",
            "268:\tlearn: 40.1739649\ttotal: 637ms\tremaining: 1.73s\n",
            "269:\tlearn: 40.1438573\ttotal: 638ms\tremaining: 1.72s\n",
            "270:\tlearn: 40.0720512\ttotal: 639ms\tremaining: 1.72s\n",
            "271:\tlearn: 40.0466060\ttotal: 640ms\tremaining: 1.71s\n",
            "272:\tlearn: 40.0128188\ttotal: 642ms\tremaining: 1.71s\n",
            "273:\tlearn: 39.9994292\ttotal: 644ms\tremaining: 1.71s\n",
            "274:\tlearn: 39.9743197\ttotal: 646ms\tremaining: 1.7s\n",
            "275:\tlearn: 39.9477671\ttotal: 647ms\tremaining: 1.7s\n",
            "276:\tlearn: 39.9179417\ttotal: 648ms\tremaining: 1.69s\n",
            "277:\tlearn: 39.9079894\ttotal: 650ms\tremaining: 1.69s\n",
            "278:\tlearn: 39.8891823\ttotal: 652ms\tremaining: 1.69s\n",
            "279:\tlearn: 39.8761394\ttotal: 654ms\tremaining: 1.68s\n",
            "280:\tlearn: 39.8292017\ttotal: 658ms\tremaining: 1.68s\n",
            "281:\tlearn: 39.7766501\ttotal: 660ms\tremaining: 1.68s\n",
            "282:\tlearn: 39.7176823\ttotal: 662ms\tremaining: 1.68s\n",
            "283:\tlearn: 39.7072529\ttotal: 668ms\tremaining: 1.68s\n",
            "284:\tlearn: 39.6403435\ttotal: 669ms\tremaining: 1.68s\n",
            "285:\tlearn: 39.6172303\ttotal: 670ms\tremaining: 1.67s\n",
            "286:\tlearn: 39.5581251\ttotal: 671ms\tremaining: 1.67s\n",
            "287:\tlearn: 39.4976941\ttotal: 677ms\tremaining: 1.67s\n",
            "288:\tlearn: 39.4406573\ttotal: 679ms\tremaining: 1.67s\n",
            "289:\tlearn: 39.3862856\ttotal: 680ms\tremaining: 1.66s\n",
            "290:\tlearn: 39.3490250\ttotal: 681ms\tremaining: 1.66s\n",
            "291:\tlearn: 39.3034674\ttotal: 682ms\tremaining: 1.65s\n",
            "292:\tlearn: 39.2597600\ttotal: 682ms\tremaining: 1.65s\n",
            "293:\tlearn: 39.2290517\ttotal: 688ms\tremaining: 1.65s\n",
            "294:\tlearn: 39.2045223\ttotal: 689ms\tremaining: 1.65s\n",
            "295:\tlearn: 39.1698151\ttotal: 690ms\tremaining: 1.64s\n",
            "296:\tlearn: 39.1296900\ttotal: 691ms\tremaining: 1.63s\n",
            "297:\tlearn: 39.0615655\ttotal: 691ms\tremaining: 1.63s\n",
            "298:\tlearn: 39.0170914\ttotal: 692ms\tremaining: 1.62s\n",
            "299:\tlearn: 39.0100010\ttotal: 693ms\tremaining: 1.62s\n",
            "300:\tlearn: 38.9396459\ttotal: 698ms\tremaining: 1.62s\n",
            "301:\tlearn: 38.9163080\ttotal: 699ms\tremaining: 1.62s\n",
            "302:\tlearn: 38.8748013\ttotal: 701ms\tremaining: 1.61s\n",
            "303:\tlearn: 38.8619579\ttotal: 706ms\tremaining: 1.62s\n",
            "304:\tlearn: 38.8308470\ttotal: 707ms\tremaining: 1.61s\n",
            "305:\tlearn: 38.8164418\ttotal: 708ms\tremaining: 1.6s\n",
            "306:\tlearn: 38.7765336\ttotal: 709ms\tremaining: 1.6s\n",
            "307:\tlearn: 38.7640496\ttotal: 709ms\tremaining: 1.59s\n",
            "308:\tlearn: 38.7312197\ttotal: 710ms\tremaining: 1.59s\n",
            "309:\tlearn: 38.6735714\ttotal: 711ms\tremaining: 1.58s\n",
            "310:\tlearn: 38.6638542\ttotal: 712ms\tremaining: 1.58s\n",
            "311:\tlearn: 38.6252189\ttotal: 713ms\tremaining: 1.57s\n",
            "312:\tlearn: 38.6106489\ttotal: 719ms\tremaining: 1.58s\n",
            "313:\tlearn: 38.5865501\ttotal: 720ms\tremaining: 1.57s\n",
            "314:\tlearn: 38.5644628\ttotal: 721ms\tremaining: 1.57s\n",
            "315:\tlearn: 38.5024318\ttotal: 722ms\tremaining: 1.56s\n",
            "316:\tlearn: 38.4844470\ttotal: 723ms\tremaining: 1.56s\n",
            "317:\tlearn: 38.4647767\ttotal: 724ms\tremaining: 1.55s\n",
            "318:\tlearn: 38.4316458\ttotal: 725ms\tremaining: 1.55s\n",
            "319:\tlearn: 38.3856500\ttotal: 726ms\tremaining: 1.54s\n",
            "320:\tlearn: 38.3479993\ttotal: 731ms\tremaining: 1.54s\n",
            "321:\tlearn: 38.3271481\ttotal: 732ms\tremaining: 1.54s\n",
            "322:\tlearn: 38.3121812\ttotal: 735ms\tremaining: 1.54s\n",
            "323:\tlearn: 38.2787020\ttotal: 735ms\tremaining: 1.53s\n",
            "324:\tlearn: 38.2537163\ttotal: 736ms\tremaining: 1.53s\n",
            "325:\tlearn: 38.2399115\ttotal: 739ms\tremaining: 1.53s\n",
            "326:\tlearn: 38.2198213\ttotal: 740ms\tremaining: 1.52s\n",
            "327:\tlearn: 38.2118298\ttotal: 742ms\tremaining: 1.52s\n",
            "328:\tlearn: 38.2006841\ttotal: 743ms\tremaining: 1.51s\n",
            "329:\tlearn: 38.1683073\ttotal: 744ms\tremaining: 1.51s\n",
            "330:\tlearn: 38.1079563\ttotal: 745ms\tremaining: 1.5s\n",
            "331:\tlearn: 38.0604858\ttotal: 750ms\tremaining: 1.51s\n",
            "332:\tlearn: 38.0143791\ttotal: 751ms\tremaining: 1.5s\n",
            "333:\tlearn: 37.9457491\ttotal: 752ms\tremaining: 1.5s\n",
            "334:\tlearn: 37.9285791\ttotal: 753ms\tremaining: 1.49s\n",
            "335:\tlearn: 37.8762794\ttotal: 754ms\tremaining: 1.49s\n",
            "336:\tlearn: 37.8547376\ttotal: 762ms\tremaining: 1.5s\n",
            "337:\tlearn: 37.8453619\ttotal: 763ms\tremaining: 1.49s\n",
            "338:\tlearn: 37.7979106\ttotal: 766ms\tremaining: 1.49s\n",
            "339:\tlearn: 37.7524550\ttotal: 769ms\tremaining: 1.49s\n",
            "340:\tlearn: 37.7135690\ttotal: 770ms\tremaining: 1.49s\n",
            "341:\tlearn: 37.6671901\ttotal: 771ms\tremaining: 1.48s\n",
            "342:\tlearn: 37.6576081\ttotal: 772ms\tremaining: 1.48s\n",
            "343:\tlearn: 37.6091472\ttotal: 774ms\tremaining: 1.48s\n",
            "344:\tlearn: 37.5817573\ttotal: 781ms\tremaining: 1.48s\n",
            "345:\tlearn: 37.5302554\ttotal: 786ms\tremaining: 1.49s\n",
            "346:\tlearn: 37.4975323\ttotal: 787ms\tremaining: 1.48s\n",
            "347:\tlearn: 37.4747430\ttotal: 790ms\tremaining: 1.48s\n",
            "348:\tlearn: 37.4301646\ttotal: 794ms\tremaining: 1.48s\n",
            "349:\tlearn: 37.4221624\ttotal: 795ms\tremaining: 1.48s\n",
            "350:\tlearn: 37.4207120\ttotal: 796ms\tremaining: 1.47s\n",
            "351:\tlearn: 37.3846699\ttotal: 797ms\tremaining: 1.47s\n",
            "352:\tlearn: 37.3281953\ttotal: 803ms\tremaining: 1.47s\n",
            "353:\tlearn: 37.3087975\ttotal: 804ms\tremaining: 1.47s\n",
            "354:\tlearn: 37.2510681\ttotal: 805ms\tremaining: 1.46s\n",
            "355:\tlearn: 37.2219189\ttotal: 808ms\tremaining: 1.46s\n",
            "356:\tlearn: 37.1899240\ttotal: 811ms\tremaining: 1.46s\n",
            "357:\tlearn: 37.1693165\ttotal: 812ms\tremaining: 1.46s\n",
            "358:\tlearn: 37.1555217\ttotal: 812ms\tremaining: 1.45s\n",
            "359:\tlearn: 37.1448846\ttotal: 823ms\tremaining: 1.46s\n",
            "360:\tlearn: 37.1015055\ttotal: 828ms\tremaining: 1.47s\n",
            "361:\tlearn: 37.0580386\ttotal: 829ms\tremaining: 1.46s\n",
            "362:\tlearn: 37.0235265\ttotal: 830ms\tremaining: 1.46s\n",
            "363:\tlearn: 37.0183656\ttotal: 831ms\tremaining: 1.45s\n",
            "364:\tlearn: 37.0085703\ttotal: 832ms\tremaining: 1.45s\n",
            "365:\tlearn: 36.9539184\ttotal: 833ms\tremaining: 1.44s\n",
            "366:\tlearn: 36.9343128\ttotal: 834ms\tremaining: 1.44s\n",
            "367:\tlearn: 36.9241332\ttotal: 837ms\tremaining: 1.44s\n",
            "368:\tlearn: 36.8644031\ttotal: 838ms\tremaining: 1.43s\n",
            "369:\tlearn: 36.8042882\ttotal: 839ms\tremaining: 1.43s\n",
            "370:\tlearn: 36.7825980\ttotal: 844ms\tremaining: 1.43s\n",
            "371:\tlearn: 36.7482951\ttotal: 845ms\tremaining: 1.43s\n",
            "372:\tlearn: 36.7325578\ttotal: 846ms\tremaining: 1.42s\n",
            "373:\tlearn: 36.7244321\ttotal: 847ms\tremaining: 1.42s\n",
            "374:\tlearn: 36.6614558\ttotal: 848ms\tremaining: 1.41s\n",
            "375:\tlearn: 36.6534911\ttotal: 849ms\tremaining: 1.41s\n",
            "376:\tlearn: 36.6228495\ttotal: 850ms\tremaining: 1.4s\n",
            "377:\tlearn: 36.5936463\ttotal: 851ms\tremaining: 1.4s\n",
            "378:\tlearn: 36.5788056\ttotal: 852ms\tremaining: 1.4s\n",
            "379:\tlearn: 36.5625750\ttotal: 857ms\tremaining: 1.4s\n",
            "380:\tlearn: 36.5550114\ttotal: 859ms\tremaining: 1.4s\n",
            "381:\tlearn: 36.5079522\ttotal: 860ms\tremaining: 1.39s\n",
            "382:\tlearn: 36.4901458\ttotal: 861ms\tremaining: 1.39s\n",
            "383:\tlearn: 36.4773820\ttotal: 862ms\tremaining: 1.38s\n",
            "384:\tlearn: 36.4524147\ttotal: 869ms\tremaining: 1.39s\n",
            "385:\tlearn: 36.4249840\ttotal: 871ms\tremaining: 1.39s\n",
            "386:\tlearn: 36.3809718\ttotal: 872ms\tremaining: 1.38s\n",
            "387:\tlearn: 36.3654493\ttotal: 877ms\tremaining: 1.38s\n",
            "388:\tlearn: 36.3530967\ttotal: 878ms\tremaining: 1.38s\n",
            "389:\tlearn: 36.3070592\ttotal: 879ms\tremaining: 1.37s\n",
            "390:\tlearn: 36.2422581\ttotal: 880ms\tremaining: 1.37s\n",
            "391:\tlearn: 36.2217524\ttotal: 881ms\tremaining: 1.36s\n",
            "392:\tlearn: 36.2043784\ttotal: 881ms\tremaining: 1.36s\n",
            "393:\tlearn: 36.1647586\ttotal: 882ms\tremaining: 1.36s\n",
            "394:\tlearn: 36.1444266\ttotal: 883ms\tremaining: 1.35s\n",
            "395:\tlearn: 36.1266723\ttotal: 889ms\tremaining: 1.35s\n",
            "396:\tlearn: 36.1011942\ttotal: 890ms\tremaining: 1.35s\n",
            "397:\tlearn: 36.0759564\ttotal: 890ms\tremaining: 1.35s\n",
            "398:\tlearn: 36.0497084\ttotal: 891ms\tremaining: 1.34s\n",
            "399:\tlearn: 36.0266499\ttotal: 892ms\tremaining: 1.34s\n",
            "400:\tlearn: 36.0208397\ttotal: 893ms\tremaining: 1.33s\n",
            "401:\tlearn: 36.0086751\ttotal: 894ms\tremaining: 1.33s\n",
            "402:\tlearn: 35.9617140\ttotal: 895ms\tremaining: 1.32s\n",
            "403:\tlearn: 35.9240834\ttotal: 900ms\tremaining: 1.33s\n",
            "404:\tlearn: 35.8601974\ttotal: 901ms\tremaining: 1.32s\n",
            "405:\tlearn: 35.8431778\ttotal: 902ms\tremaining: 1.32s\n",
            "406:\tlearn: 35.8267218\ttotal: 903ms\tremaining: 1.32s\n",
            "407:\tlearn: 35.7978972\ttotal: 904ms\tremaining: 1.31s\n",
            "408:\tlearn: 35.7754737\ttotal: 905ms\tremaining: 1.31s\n",
            "409:\tlearn: 35.7588761\ttotal: 906ms\tremaining: 1.3s\n",
            "410:\tlearn: 35.7481175\ttotal: 907ms\tremaining: 1.3s\n",
            "411:\tlearn: 35.7067357\ttotal: 912ms\tremaining: 1.3s\n",
            "412:\tlearn: 35.6782974\ttotal: 916ms\tremaining: 1.3s\n",
            "413:\tlearn: 35.6316022\ttotal: 917ms\tremaining: 1.3s\n",
            "414:\tlearn: 35.6174783\ttotal: 918ms\tremaining: 1.29s\n",
            "415:\tlearn: 35.5895449\ttotal: 919ms\tremaining: 1.29s\n",
            "416:\tlearn: 35.5414059\ttotal: 920ms\tremaining: 1.29s\n",
            "417:\tlearn: 35.4983572\ttotal: 921ms\tremaining: 1.28s\n",
            "418:\tlearn: 35.4457584\ttotal: 922ms\tremaining: 1.28s\n",
            "419:\tlearn: 35.4358548\ttotal: 927ms\tremaining: 1.28s\n",
            "420:\tlearn: 35.4097784\ttotal: 928ms\tremaining: 1.28s\n",
            "421:\tlearn: 35.4018443\ttotal: 929ms\tremaining: 1.27s\n",
            "422:\tlearn: 35.3599548\ttotal: 930ms\tremaining: 1.27s\n",
            "423:\tlearn: 35.3421221\ttotal: 931ms\tremaining: 1.26s\n",
            "424:\tlearn: 35.2979917\ttotal: 932ms\tremaining: 1.26s\n",
            "425:\tlearn: 35.2904048\ttotal: 932ms\tremaining: 1.26s\n",
            "426:\tlearn: 35.2765827\ttotal: 933ms\tremaining: 1.25s\n",
            "427:\tlearn: 35.2542861\ttotal: 940ms\tremaining: 1.25s\n",
            "428:\tlearn: 35.2400249\ttotal: 941ms\tremaining: 1.25s\n",
            "429:\tlearn: 35.2298013\ttotal: 942ms\tremaining: 1.25s\n",
            "430:\tlearn: 35.1977697\ttotal: 943ms\tremaining: 1.24s\n",
            "431:\tlearn: 35.1828011\ttotal: 943ms\tremaining: 1.24s\n",
            "432:\tlearn: 35.1259994\ttotal: 944ms\tremaining: 1.24s\n",
            "433:\tlearn: 35.1103818\ttotal: 945ms\tremaining: 1.23s\n",
            "434:\tlearn: 35.0739203\ttotal: 946ms\tremaining: 1.23s\n",
            "435:\tlearn: 35.0209610\ttotal: 952ms\tremaining: 1.23s\n",
            "436:\tlearn: 34.9635803\ttotal: 953ms\tremaining: 1.23s\n",
            "437:\tlearn: 34.9256628\ttotal: 954ms\tremaining: 1.22s\n",
            "438:\tlearn: 34.8953299\ttotal: 955ms\tremaining: 1.22s\n",
            "439:\tlearn: 34.8709280\ttotal: 956ms\tremaining: 1.22s\n",
            "440:\tlearn: 34.8554654\ttotal: 957ms\tremaining: 1.21s\n",
            "441:\tlearn: 34.8500193\ttotal: 960ms\tremaining: 1.21s\n",
            "442:\tlearn: 34.8298320\ttotal: 963ms\tremaining: 1.21s\n",
            "443:\tlearn: 34.7941867\ttotal: 964ms\tremaining: 1.21s\n",
            "444:\tlearn: 34.7335447\ttotal: 966ms\tremaining: 1.2s\n",
            "445:\tlearn: 34.6753983\ttotal: 968ms\tremaining: 1.2s\n",
            "446:\tlearn: 34.6511409\ttotal: 976ms\tremaining: 1.21s\n",
            "447:\tlearn: 34.6154399\ttotal: 978ms\tremaining: 1.2s\n",
            "448:\tlearn: 34.5718376\ttotal: 979ms\tremaining: 1.2s\n",
            "449:\tlearn: 34.5479227\ttotal: 981ms\tremaining: 1.2s\n",
            "450:\tlearn: 34.5210262\ttotal: 983ms\tremaining: 1.2s\n",
            "451:\tlearn: 34.4812274\ttotal: 984ms\tremaining: 1.19s\n",
            "452:\tlearn: 34.4394467\ttotal: 986ms\tremaining: 1.19s\n",
            "453:\tlearn: 34.4007597\ttotal: 987ms\tremaining: 1.19s\n",
            "454:\tlearn: 34.3430437\ttotal: 989ms\tremaining: 1.18s\n",
            "455:\tlearn: 34.3378204\ttotal: 990ms\tremaining: 1.18s\n",
            "456:\tlearn: 34.2830239\ttotal: 992ms\tremaining: 1.18s\n",
            "457:\tlearn: 34.2420606\ttotal: 993ms\tremaining: 1.18s\n",
            "458:\tlearn: 34.2259256\ttotal: 995ms\tremaining: 1.17s\n",
            "459:\tlearn: 34.1841792\ttotal: 996ms\tremaining: 1.17s\n",
            "460:\tlearn: 34.1430067\ttotal: 998ms\tremaining: 1.17s\n",
            "461:\tlearn: 34.1239213\ttotal: 999ms\tremaining: 1.16s\n",
            "462:\tlearn: 34.0954866\ttotal: 1s\tremaining: 1.16s\n",
            "463:\tlearn: 34.0675404\ttotal: 1s\tremaining: 1.16s\n",
            "464:\tlearn: 34.0356169\ttotal: 1s\tremaining: 1.16s\n",
            "465:\tlearn: 33.9508014\ttotal: 1s\tremaining: 1.15s\n",
            "466:\tlearn: 33.9127217\ttotal: 1.01s\tremaining: 1.15s\n",
            "467:\tlearn: 33.8584512\ttotal: 1.01s\tremaining: 1.15s\n",
            "468:\tlearn: 33.8562359\ttotal: 1.01s\tremaining: 1.14s\n",
            "469:\tlearn: 33.8215846\ttotal: 1.01s\tremaining: 1.14s\n",
            "470:\tlearn: 33.8076721\ttotal: 1.01s\tremaining: 1.14s\n",
            "471:\tlearn: 33.7671489\ttotal: 1.01s\tremaining: 1.13s\n",
            "472:\tlearn: 33.7298156\ttotal: 1.02s\tremaining: 1.13s\n",
            "473:\tlearn: 33.7131617\ttotal: 1.02s\tremaining: 1.13s\n",
            "474:\tlearn: 33.7066195\ttotal: 1.02s\tremaining: 1.13s\n",
            "475:\tlearn: 33.6952567\ttotal: 1.02s\tremaining: 1.12s\n",
            "476:\tlearn: 33.6781098\ttotal: 1.02s\tremaining: 1.12s\n",
            "477:\tlearn: 33.6584247\ttotal: 1.02s\tremaining: 1.12s\n",
            "478:\tlearn: 33.6231301\ttotal: 1.02s\tremaining: 1.11s\n",
            "479:\tlearn: 33.6213720\ttotal: 1.03s\tremaining: 1.11s\n",
            "480:\tlearn: 33.6139147\ttotal: 1.03s\tremaining: 1.11s\n",
            "481:\tlearn: 33.5998674\ttotal: 1.03s\tremaining: 1.11s\n",
            "482:\tlearn: 33.5719861\ttotal: 1.03s\tremaining: 1.1s\n",
            "483:\tlearn: 33.5637554\ttotal: 1.03s\tremaining: 1.1s\n",
            "484:\tlearn: 33.4829748\ttotal: 1.03s\tremaining: 1.1s\n",
            "485:\tlearn: 33.4664409\ttotal: 1.04s\tremaining: 1.11s\n",
            "486:\tlearn: 33.4523200\ttotal: 1.05s\tremaining: 1.1s\n",
            "487:\tlearn: 33.4440612\ttotal: 1.05s\tremaining: 1.1s\n",
            "488:\tlearn: 33.4287204\ttotal: 1.05s\tremaining: 1.1s\n",
            "489:\tlearn: 33.4233602\ttotal: 1.05s\tremaining: 1.1s\n",
            "490:\tlearn: 33.3932810\ttotal: 1.05s\tremaining: 1.09s\n",
            "491:\tlearn: 33.3835470\ttotal: 1.06s\tremaining: 1.09s\n",
            "492:\tlearn: 33.3767166\ttotal: 1.06s\tremaining: 1.09s\n",
            "493:\tlearn: 33.3482904\ttotal: 1.06s\tremaining: 1.08s\n",
            "494:\tlearn: 33.3276111\ttotal: 1.06s\tremaining: 1.08s\n",
            "495:\tlearn: 33.3215702\ttotal: 1.06s\tremaining: 1.08s\n",
            "496:\tlearn: 33.3154216\ttotal: 1.06s\tremaining: 1.08s\n",
            "497:\tlearn: 33.2967386\ttotal: 1.06s\tremaining: 1.07s\n",
            "498:\tlearn: 33.2187615\ttotal: 1.07s\tremaining: 1.07s\n",
            "499:\tlearn: 33.1607036\ttotal: 1.07s\tremaining: 1.07s\n",
            "500:\tlearn: 33.1480268\ttotal: 1.07s\tremaining: 1.07s\n",
            "501:\tlearn: 33.1334059\ttotal: 1.07s\tremaining: 1.06s\n",
            "502:\tlearn: 33.1231426\ttotal: 1.07s\tremaining: 1.06s\n",
            "503:\tlearn: 33.1110153\ttotal: 1.07s\tremaining: 1.06s\n",
            "504:\tlearn: 33.0865459\ttotal: 1.08s\tremaining: 1.06s\n",
            "505:\tlearn: 33.0595320\ttotal: 1.08s\tremaining: 1.05s\n",
            "506:\tlearn: 33.0473450\ttotal: 1.08s\tremaining: 1.05s\n",
            "507:\tlearn: 33.0095657\ttotal: 1.08s\tremaining: 1.05s\n",
            "508:\tlearn: 32.9633277\ttotal: 1.08s\tremaining: 1.05s\n",
            "509:\tlearn: 32.9363762\ttotal: 1.09s\tremaining: 1.04s\n",
            "510:\tlearn: 32.9235222\ttotal: 1.09s\tremaining: 1.04s\n",
            "511:\tlearn: 32.9142186\ttotal: 1.09s\tremaining: 1.04s\n",
            "512:\tlearn: 32.9082667\ttotal: 1.09s\tremaining: 1.04s\n",
            "513:\tlearn: 32.8968550\ttotal: 1.09s\tremaining: 1.03s\n",
            "514:\tlearn: 32.8891928\ttotal: 1.09s\tremaining: 1.03s\n",
            "515:\tlearn: 32.8833845\ttotal: 1.1s\tremaining: 1.03s\n",
            "516:\tlearn: 32.8777252\ttotal: 1.1s\tremaining: 1.03s\n",
            "517:\tlearn: 32.8408055\ttotal: 1.1s\tremaining: 1.03s\n",
            "518:\tlearn: 32.8080471\ttotal: 1.1s\tremaining: 1.02s\n",
            "519:\tlearn: 32.8039561\ttotal: 1.11s\tremaining: 1.02s\n",
            "520:\tlearn: 32.7998292\ttotal: 1.11s\tremaining: 1.02s\n",
            "521:\tlearn: 32.7622258\ttotal: 1.11s\tremaining: 1.02s\n",
            "522:\tlearn: 32.7216277\ttotal: 1.11s\tremaining: 1.01s\n",
            "523:\tlearn: 32.6936972\ttotal: 1.11s\tremaining: 1.01s\n",
            "524:\tlearn: 32.6639301\ttotal: 1.11s\tremaining: 1.01s\n",
            "525:\tlearn: 32.6599235\ttotal: 1.12s\tremaining: 1s\n",
            "526:\tlearn: 32.6317115\ttotal: 1.12s\tremaining: 1s\n",
            "527:\tlearn: 32.6038172\ttotal: 1.12s\tremaining: 1s\n",
            "528:\tlearn: 32.5848133\ttotal: 1.12s\tremaining: 1s\n",
            "529:\tlearn: 32.5726660\ttotal: 1.13s\tremaining: 998ms\n",
            "530:\tlearn: 32.5355198\ttotal: 1.13s\tremaining: 996ms\n",
            "531:\tlearn: 32.5213523\ttotal: 1.13s\tremaining: 993ms\n",
            "532:\tlearn: 32.4985896\ttotal: 1.13s\tremaining: 991ms\n",
            "533:\tlearn: 32.4554390\ttotal: 1.13s\tremaining: 988ms\n",
            "534:\tlearn: 32.4489026\ttotal: 1.13s\tremaining: 985ms\n",
            "535:\tlearn: 32.4058370\ttotal: 1.14s\tremaining: 983ms\n",
            "536:\tlearn: 32.3816831\ttotal: 1.14s\tremaining: 981ms\n",
            "537:\tlearn: 32.3806837\ttotal: 1.14s\tremaining: 977ms\n",
            "538:\tlearn: 32.3601181\ttotal: 1.14s\tremaining: 974ms\n",
            "539:\tlearn: 32.3414665\ttotal: 1.14s\tremaining: 971ms\n",
            "540:\tlearn: 32.3141345\ttotal: 1.14s\tremaining: 969ms\n",
            "541:\tlearn: 32.2736787\ttotal: 1.14s\tremaining: 967ms\n",
            "542:\tlearn: 32.2500626\ttotal: 1.15s\tremaining: 964ms\n",
            "543:\tlearn: 32.2039772\ttotal: 1.15s\tremaining: 963ms\n",
            "544:\tlearn: 32.1521175\ttotal: 1.15s\tremaining: 961ms\n",
            "545:\tlearn: 32.1399054\ttotal: 1.15s\tremaining: 958ms\n",
            "546:\tlearn: 32.0909949\ttotal: 1.15s\tremaining: 956ms\n",
            "547:\tlearn: 32.0660120\ttotal: 1.16s\tremaining: 954ms\n",
            "548:\tlearn: 32.0530801\ttotal: 1.16s\tremaining: 951ms\n",
            "549:\tlearn: 32.0047388\ttotal: 1.16s\tremaining: 948ms\n",
            "550:\tlearn: 32.0011774\ttotal: 1.16s\tremaining: 946ms\n",
            "551:\tlearn: 31.9910573\ttotal: 1.16s\tremaining: 944ms\n",
            "552:\tlearn: 31.9724846\ttotal: 1.16s\tremaining: 941ms\n",
            "553:\tlearn: 31.9589916\ttotal: 1.17s\tremaining: 938ms\n",
            "554:\tlearn: 31.9479006\ttotal: 1.17s\tremaining: 936ms\n",
            "555:\tlearn: 31.9339104\ttotal: 1.17s\tremaining: 933ms\n",
            "556:\tlearn: 31.9218589\ttotal: 1.17s\tremaining: 929ms\n",
            "557:\tlearn: 31.8872829\ttotal: 1.17s\tremaining: 926ms\n",
            "558:\tlearn: 31.8773399\ttotal: 1.17s\tremaining: 923ms\n",
            "559:\tlearn: 31.8721696\ttotal: 1.17s\tremaining: 920ms\n",
            "560:\tlearn: 31.8356110\ttotal: 1.17s\tremaining: 917ms\n",
            "561:\tlearn: 31.8223413\ttotal: 1.17s\tremaining: 914ms\n",
            "562:\tlearn: 31.7307094\ttotal: 1.17s\tremaining: 910ms\n",
            "563:\tlearn: 31.6577451\ttotal: 1.17s\tremaining: 907ms\n",
            "564:\tlearn: 31.6533421\ttotal: 1.17s\tremaining: 904ms\n",
            "565:\tlearn: 31.6458121\ttotal: 1.17s\tremaining: 901ms\n",
            "566:\tlearn: 31.5945783\ttotal: 1.18s\tremaining: 898ms\n",
            "567:\tlearn: 31.5855624\ttotal: 1.18s\tremaining: 895ms\n",
            "568:\tlearn: 31.5670927\ttotal: 1.18s\tremaining: 892ms\n",
            "569:\tlearn: 31.4971586\ttotal: 1.18s\tremaining: 889ms\n",
            "570:\tlearn: 31.4902269\ttotal: 1.18s\tremaining: 885ms\n",
            "571:\tlearn: 31.4802635\ttotal: 1.18s\tremaining: 882ms\n",
            "572:\tlearn: 31.4662633\ttotal: 1.18s\tremaining: 879ms\n",
            "573:\tlearn: 31.4598864\ttotal: 1.18s\tremaining: 876ms\n",
            "574:\tlearn: 31.4292936\ttotal: 1.18s\tremaining: 874ms\n",
            "575:\tlearn: 31.4226091\ttotal: 1.18s\tremaining: 871ms\n",
            "576:\tlearn: 31.4007456\ttotal: 1.18s\tremaining: 868ms\n",
            "577:\tlearn: 31.3955926\ttotal: 1.18s\tremaining: 865ms\n",
            "578:\tlearn: 31.3664616\ttotal: 1.19s\tremaining: 862ms\n",
            "579:\tlearn: 31.3580389\ttotal: 1.19s\tremaining: 859ms\n",
            "580:\tlearn: 31.3517936\ttotal: 1.19s\tremaining: 856ms\n",
            "581:\tlearn: 31.3375621\ttotal: 1.19s\tremaining: 853ms\n",
            "582:\tlearn: 31.2934416\ttotal: 1.19s\tremaining: 849ms\n",
            "583:\tlearn: 31.2491171\ttotal: 1.19s\tremaining: 847ms\n",
            "584:\tlearn: 31.2212390\ttotal: 1.19s\tremaining: 844ms\n",
            "585:\tlearn: 31.2100086\ttotal: 1.19s\tremaining: 840ms\n",
            "586:\tlearn: 31.1526056\ttotal: 1.19s\tremaining: 837ms\n",
            "587:\tlearn: 31.1236816\ttotal: 1.19s\tremaining: 835ms\n",
            "588:\tlearn: 31.1184964\ttotal: 1.19s\tremaining: 832ms\n",
            "589:\tlearn: 31.1137133\ttotal: 1.19s\tremaining: 828ms\n",
            "590:\tlearn: 31.1033371\ttotal: 1.19s\tremaining: 825ms\n",
            "591:\tlearn: 31.0928118\ttotal: 1.19s\tremaining: 823ms\n",
            "592:\tlearn: 31.0391054\ttotal: 1.19s\tremaining: 820ms\n",
            "593:\tlearn: 31.0296752\ttotal: 1.2s\tremaining: 817ms\n",
            "594:\tlearn: 31.0202049\ttotal: 1.2s\tremaining: 814ms\n",
            "595:\tlearn: 31.0109221\ttotal: 1.2s\tremaining: 811ms\n",
            "596:\tlearn: 31.0070861\ttotal: 1.2s\tremaining: 808ms\n",
            "597:\tlearn: 30.9839148\ttotal: 1.2s\tremaining: 806ms\n",
            "598:\tlearn: 30.9736833\ttotal: 1.2s\tremaining: 803ms\n",
            "599:\tlearn: 30.9494080\ttotal: 1.2s\tremaining: 800ms\n",
            "600:\tlearn: 30.9405148\ttotal: 1.2s\tremaining: 797ms\n",
            "601:\tlearn: 30.9350632\ttotal: 1.2s\tremaining: 794ms\n",
            "602:\tlearn: 30.9194096\ttotal: 1.2s\tremaining: 791ms\n",
            "603:\tlearn: 30.9158928\ttotal: 1.2s\tremaining: 788ms\n",
            "604:\tlearn: 30.8769157\ttotal: 1.2s\tremaining: 786ms\n",
            "605:\tlearn: 30.8679097\ttotal: 1.2s\tremaining: 783ms\n",
            "606:\tlearn: 30.8344793\ttotal: 1.21s\tremaining: 780ms\n",
            "607:\tlearn: 30.7796100\ttotal: 1.21s\tremaining: 777ms\n",
            "608:\tlearn: 30.7188768\ttotal: 1.21s\tremaining: 774ms\n",
            "609:\tlearn: 30.7087152\ttotal: 1.21s\tremaining: 772ms\n",
            "610:\tlearn: 30.6389480\ttotal: 1.21s\tremaining: 769ms\n",
            "611:\tlearn: 30.6010807\ttotal: 1.22s\tremaining: 771ms\n",
            "612:\tlearn: 30.5672160\ttotal: 1.22s\tremaining: 768ms\n",
            "613:\tlearn: 30.5629478\ttotal: 1.22s\tremaining: 765ms\n",
            "614:\tlearn: 30.5536363\ttotal: 1.22s\tremaining: 762ms\n",
            "615:\tlearn: 30.4672292\ttotal: 1.22s\tremaining: 760ms\n",
            "616:\tlearn: 30.4386279\ttotal: 1.22s\tremaining: 757ms\n",
            "617:\tlearn: 30.4315228\ttotal: 1.22s\tremaining: 754ms\n",
            "618:\tlearn: 30.4246821\ttotal: 1.22s\tremaining: 751ms\n",
            "619:\tlearn: 30.4149463\ttotal: 1.22s\tremaining: 748ms\n",
            "620:\tlearn: 30.3908070\ttotal: 1.22s\tremaining: 745ms\n",
            "621:\tlearn: 30.3865392\ttotal: 1.22s\tremaining: 743ms\n",
            "622:\tlearn: 30.3732515\ttotal: 1.22s\tremaining: 740ms\n",
            "623:\tlearn: 30.3628282\ttotal: 1.23s\tremaining: 739ms\n",
            "624:\tlearn: 30.3586703\ttotal: 1.23s\tremaining: 736ms\n",
            "625:\tlearn: 30.3366864\ttotal: 1.23s\tremaining: 733ms\n",
            "626:\tlearn: 30.3282933\ttotal: 1.23s\tremaining: 731ms\n",
            "627:\tlearn: 30.3056921\ttotal: 1.23s\tremaining: 728ms\n",
            "628:\tlearn: 30.2733035\ttotal: 1.23s\tremaining: 725ms\n",
            "629:\tlearn: 30.2203545\ttotal: 1.23s\tremaining: 722ms\n",
            "630:\tlearn: 30.2087482\ttotal: 1.23s\tremaining: 720ms\n",
            "631:\tlearn: 30.2026920\ttotal: 1.24s\tremaining: 720ms\n",
            "632:\tlearn: 30.1643878\ttotal: 1.24s\tremaining: 717ms\n",
            "633:\tlearn: 30.1555278\ttotal: 1.24s\tremaining: 714ms\n",
            "634:\tlearn: 30.1500825\ttotal: 1.24s\tremaining: 711ms\n",
            "635:\tlearn: 30.1328106\ttotal: 1.24s\tremaining: 709ms\n",
            "636:\tlearn: 30.1060209\ttotal: 1.24s\tremaining: 709ms\n",
            "637:\tlearn: 30.0872352\ttotal: 1.24s\tremaining: 706ms\n",
            "638:\tlearn: 30.0720954\ttotal: 1.24s\tremaining: 703ms\n",
            "639:\tlearn: 30.0514010\ttotal: 1.25s\tremaining: 701ms\n",
            "640:\tlearn: 30.0467339\ttotal: 1.25s\tremaining: 698ms\n",
            "641:\tlearn: 30.0356245\ttotal: 1.25s\tremaining: 697ms\n",
            "642:\tlearn: 30.0008037\ttotal: 1.25s\tremaining: 695ms\n",
            "643:\tlearn: 29.9514374\ttotal: 1.25s\tremaining: 692ms\n",
            "644:\tlearn: 29.9195717\ttotal: 1.25s\tremaining: 690ms\n",
            "645:\tlearn: 29.8960511\ttotal: 1.26s\tremaining: 690ms\n",
            "646:\tlearn: 29.8737994\ttotal: 1.26s\tremaining: 687ms\n",
            "647:\tlearn: 29.8562726\ttotal: 1.26s\tremaining: 684ms\n",
            "648:\tlearn: 29.8523550\ttotal: 1.26s\tremaining: 682ms\n",
            "649:\tlearn: 29.8269131\ttotal: 1.26s\tremaining: 679ms\n",
            "650:\tlearn: 29.8146779\ttotal: 1.26s\tremaining: 676ms\n",
            "651:\tlearn: 29.7882533\ttotal: 1.26s\tremaining: 674ms\n",
            "652:\tlearn: 29.7204184\ttotal: 1.26s\tremaining: 671ms\n",
            "653:\tlearn: 29.6841937\ttotal: 1.26s\tremaining: 668ms\n",
            "654:\tlearn: 29.6804960\ttotal: 1.26s\tremaining: 666ms\n",
            "655:\tlearn: 29.6588354\ttotal: 1.26s\tremaining: 663ms\n",
            "656:\tlearn: 29.6561185\ttotal: 1.26s\tremaining: 660ms\n",
            "657:\tlearn: 29.6149792\ttotal: 1.26s\tremaining: 658ms\n",
            "658:\tlearn: 29.6110046\ttotal: 1.27s\tremaining: 655ms\n",
            "659:\tlearn: 29.5796838\ttotal: 1.27s\tremaining: 653ms\n",
            "660:\tlearn: 29.5404487\ttotal: 1.27s\tremaining: 650ms\n",
            "661:\tlearn: 29.5325434\ttotal: 1.27s\tremaining: 648ms\n",
            "662:\tlearn: 29.5185161\ttotal: 1.27s\tremaining: 645ms\n",
            "663:\tlearn: 29.4841691\ttotal: 1.27s\tremaining: 642ms\n",
            "664:\tlearn: 29.4535363\ttotal: 1.27s\tremaining: 640ms\n",
            "665:\tlearn: 29.4197221\ttotal: 1.27s\tremaining: 637ms\n",
            "666:\tlearn: 29.4185070\ttotal: 1.27s\tremaining: 635ms\n",
            "667:\tlearn: 29.4148067\ttotal: 1.27s\tremaining: 633ms\n",
            "668:\tlearn: 29.3824206\ttotal: 1.27s\tremaining: 630ms\n",
            "669:\tlearn: 29.3701750\ttotal: 1.27s\tremaining: 627ms\n",
            "670:\tlearn: 29.3398550\ttotal: 1.27s\tremaining: 625ms\n",
            "671:\tlearn: 29.3201833\ttotal: 1.27s\tremaining: 622ms\n",
            "672:\tlearn: 29.3063494\ttotal: 1.27s\tremaining: 620ms\n",
            "673:\tlearn: 29.2841064\ttotal: 1.28s\tremaining: 617ms\n",
            "674:\tlearn: 29.2807024\ttotal: 1.28s\tremaining: 615ms\n",
            "675:\tlearn: 29.2694168\ttotal: 1.28s\tremaining: 612ms\n",
            "676:\tlearn: 29.2272634\ttotal: 1.28s\tremaining: 610ms\n",
            "677:\tlearn: 29.2181094\ttotal: 1.28s\tremaining: 608ms\n",
            "678:\tlearn: 29.1985709\ttotal: 1.28s\tremaining: 605ms\n",
            "679:\tlearn: 29.1914796\ttotal: 1.28s\tremaining: 603ms\n",
            "680:\tlearn: 29.1804242\ttotal: 1.28s\tremaining: 600ms\n",
            "681:\tlearn: 29.1251672\ttotal: 1.28s\tremaining: 598ms\n",
            "682:\tlearn: 29.1213354\ttotal: 1.28s\tremaining: 595ms\n",
            "683:\tlearn: 29.1032551\ttotal: 1.28s\tremaining: 593ms\n",
            "684:\tlearn: 29.0892626\ttotal: 1.28s\tremaining: 590ms\n",
            "685:\tlearn: 29.0869556\ttotal: 1.28s\tremaining: 588ms\n",
            "686:\tlearn: 29.0804114\ttotal: 1.28s\tremaining: 586ms\n",
            "687:\tlearn: 29.0622441\ttotal: 1.29s\tremaining: 583ms\n",
            "688:\tlearn: 28.9854586\ttotal: 1.29s\tremaining: 581ms\n",
            "689:\tlearn: 28.9673820\ttotal: 1.29s\tremaining: 578ms\n",
            "690:\tlearn: 28.9571941\ttotal: 1.29s\tremaining: 576ms\n",
            "691:\tlearn: 28.9393437\ttotal: 1.29s\tremaining: 574ms\n",
            "692:\tlearn: 28.9367966\ttotal: 1.29s\tremaining: 571ms\n",
            "693:\tlearn: 28.9057591\ttotal: 1.29s\tremaining: 569ms\n",
            "694:\tlearn: 28.8900514\ttotal: 1.29s\tremaining: 567ms\n",
            "695:\tlearn: 28.8727921\ttotal: 1.29s\tremaining: 564ms\n",
            "696:\tlearn: 28.8673333\ttotal: 1.29s\tremaining: 562ms\n",
            "697:\tlearn: 28.8600015\ttotal: 1.29s\tremaining: 559ms\n",
            "698:\tlearn: 28.8240462\ttotal: 1.29s\tremaining: 557ms\n",
            "699:\tlearn: 28.7609332\ttotal: 1.29s\tremaining: 555ms\n",
            "700:\tlearn: 28.7521700\ttotal: 1.29s\tremaining: 552ms\n",
            "701:\tlearn: 28.7355778\ttotal: 1.29s\tremaining: 550ms\n",
            "702:\tlearn: 28.7279243\ttotal: 1.3s\tremaining: 548ms\n",
            "703:\tlearn: 28.7228778\ttotal: 1.3s\tremaining: 545ms\n",
            "704:\tlearn: 28.7020745\ttotal: 1.3s\tremaining: 543ms\n",
            "705:\tlearn: 28.6954909\ttotal: 1.3s\tremaining: 541ms\n",
            "706:\tlearn: 28.6477047\ttotal: 1.3s\tremaining: 539ms\n",
            "707:\tlearn: 28.6364931\ttotal: 1.3s\tremaining: 536ms\n",
            "708:\tlearn: 28.6158362\ttotal: 1.3s\tremaining: 534ms\n",
            "709:\tlearn: 28.5832133\ttotal: 1.3s\tremaining: 532ms\n",
            "710:\tlearn: 28.5766620\ttotal: 1.3s\tremaining: 529ms\n",
            "711:\tlearn: 28.5262729\ttotal: 1.3s\tremaining: 527ms\n",
            "712:\tlearn: 28.5002683\ttotal: 1.3s\tremaining: 525ms\n",
            "713:\tlearn: 28.4979968\ttotal: 1.3s\tremaining: 522ms\n",
            "714:\tlearn: 28.4789869\ttotal: 1.3s\tremaining: 520ms\n",
            "715:\tlearn: 28.4759196\ttotal: 1.31s\tremaining: 519ms\n",
            "716:\tlearn: 28.4527490\ttotal: 1.31s\tremaining: 516ms\n",
            "717:\tlearn: 28.4039555\ttotal: 1.31s\tremaining: 514ms\n",
            "718:\tlearn: 28.3859958\ttotal: 1.31s\tremaining: 512ms\n",
            "719:\tlearn: 28.3262441\ttotal: 1.31s\tremaining: 510ms\n",
            "720:\tlearn: 28.2978079\ttotal: 1.31s\tremaining: 507ms\n",
            "721:\tlearn: 28.2683263\ttotal: 1.31s\tremaining: 505ms\n",
            "722:\tlearn: 28.2626172\ttotal: 1.31s\tremaining: 503ms\n",
            "723:\tlearn: 28.2586094\ttotal: 1.31s\tremaining: 500ms\n",
            "724:\tlearn: 28.2217712\ttotal: 1.31s\tremaining: 498ms\n",
            "725:\tlearn: 28.1911490\ttotal: 1.31s\tremaining: 496ms\n",
            "726:\tlearn: 28.1788395\ttotal: 1.31s\tremaining: 494ms\n",
            "727:\tlearn: 28.1211709\ttotal: 1.31s\tremaining: 491ms\n",
            "728:\tlearn: 28.1201392\ttotal: 1.31s\tremaining: 489ms\n",
            "729:\tlearn: 28.1029451\ttotal: 1.32s\tremaining: 487ms\n",
            "730:\tlearn: 28.0854086\ttotal: 1.32s\tremaining: 485ms\n",
            "731:\tlearn: 28.0425794\ttotal: 1.32s\tremaining: 482ms\n",
            "732:\tlearn: 28.0186266\ttotal: 1.32s\tremaining: 480ms\n",
            "733:\tlearn: 27.9907726\ttotal: 1.32s\tremaining: 478ms\n",
            "734:\tlearn: 27.9883675\ttotal: 1.32s\tremaining: 476ms\n",
            "735:\tlearn: 27.9572469\ttotal: 1.32s\tremaining: 474ms\n",
            "736:\tlearn: 27.9250204\ttotal: 1.32s\tremaining: 471ms\n",
            "737:\tlearn: 27.9173764\ttotal: 1.32s\tremaining: 469ms\n",
            "738:\tlearn: 27.9061361\ttotal: 1.32s\tremaining: 467ms\n",
            "739:\tlearn: 27.8980615\ttotal: 1.32s\tremaining: 465ms\n",
            "740:\tlearn: 27.8793742\ttotal: 1.32s\tremaining: 463ms\n",
            "741:\tlearn: 27.8658679\ttotal: 1.32s\tremaining: 461ms\n",
            "742:\tlearn: 27.8605821\ttotal: 1.32s\tremaining: 459ms\n",
            "743:\tlearn: 27.8551098\ttotal: 1.33s\tremaining: 456ms\n",
            "744:\tlearn: 27.8506647\ttotal: 1.33s\tremaining: 454ms\n",
            "745:\tlearn: 27.8219249\ttotal: 1.33s\tremaining: 452ms\n",
            "746:\tlearn: 27.7957675\ttotal: 1.33s\tremaining: 450ms\n",
            "747:\tlearn: 27.7859118\ttotal: 1.33s\tremaining: 448ms\n",
            "748:\tlearn: 27.7568333\ttotal: 1.33s\tremaining: 446ms\n",
            "749:\tlearn: 27.7379106\ttotal: 1.33s\tremaining: 444ms\n",
            "750:\tlearn: 27.6925949\ttotal: 1.33s\tremaining: 442ms\n",
            "751:\tlearn: 27.6696317\ttotal: 1.34s\tremaining: 441ms\n",
            "752:\tlearn: 27.6580833\ttotal: 1.34s\tremaining: 439ms\n",
            "753:\tlearn: 27.6543480\ttotal: 1.34s\tremaining: 437ms\n",
            "754:\tlearn: 27.6517108\ttotal: 1.34s\tremaining: 435ms\n",
            "755:\tlearn: 27.6420544\ttotal: 1.34s\tremaining: 433ms\n",
            "756:\tlearn: 27.6216156\ttotal: 1.34s\tremaining: 430ms\n",
            "757:\tlearn: 27.5904988\ttotal: 1.34s\tremaining: 428ms\n",
            "758:\tlearn: 27.5848017\ttotal: 1.35s\tremaining: 428ms\n",
            "759:\tlearn: 27.5839935\ttotal: 1.35s\tremaining: 426ms\n",
            "760:\tlearn: 27.5617960\ttotal: 1.35s\tremaining: 424ms\n",
            "761:\tlearn: 27.5539451\ttotal: 1.35s\tremaining: 422ms\n",
            "762:\tlearn: 27.5237576\ttotal: 1.35s\tremaining: 419ms\n",
            "763:\tlearn: 27.4989795\ttotal: 1.35s\tremaining: 419ms\n",
            "764:\tlearn: 27.4820254\ttotal: 1.36s\tremaining: 417ms\n",
            "765:\tlearn: 27.4578370\ttotal: 1.36s\tremaining: 415ms\n",
            "766:\tlearn: 27.4565921\ttotal: 1.36s\tremaining: 413ms\n",
            "767:\tlearn: 27.4412771\ttotal: 1.36s\tremaining: 410ms\n",
            "768:\tlearn: 27.4392583\ttotal: 1.36s\tremaining: 410ms\n",
            "769:\tlearn: 27.4234223\ttotal: 1.37s\tremaining: 408ms\n",
            "770:\tlearn: 27.3918840\ttotal: 1.37s\tremaining: 408ms\n",
            "771:\tlearn: 27.3735632\ttotal: 1.37s\tremaining: 405ms\n",
            "772:\tlearn: 27.3425495\ttotal: 1.37s\tremaining: 403ms\n",
            "773:\tlearn: 27.3264900\ttotal: 1.37s\tremaining: 401ms\n",
            "774:\tlearn: 27.3206828\ttotal: 1.37s\tremaining: 399ms\n",
            "775:\tlearn: 27.3114861\ttotal: 1.38s\tremaining: 397ms\n",
            "776:\tlearn: 27.2569793\ttotal: 1.38s\tremaining: 396ms\n",
            "777:\tlearn: 27.2481746\ttotal: 1.38s\tremaining: 394ms\n",
            "778:\tlearn: 27.2164909\ttotal: 1.38s\tremaining: 392ms\n",
            "779:\tlearn: 27.1777436\ttotal: 1.38s\tremaining: 390ms\n",
            "780:\tlearn: 27.1545174\ttotal: 1.38s\tremaining: 388ms\n",
            "781:\tlearn: 27.1437885\ttotal: 1.38s\tremaining: 386ms\n",
            "782:\tlearn: 27.1396257\ttotal: 1.38s\tremaining: 384ms\n",
            "783:\tlearn: 27.1173462\ttotal: 1.38s\tremaining: 381ms\n",
            "784:\tlearn: 27.0920708\ttotal: 1.39s\tremaining: 379ms\n",
            "785:\tlearn: 27.0903103\ttotal: 1.39s\tremaining: 377ms\n",
            "786:\tlearn: 27.0872455\ttotal: 1.39s\tremaining: 375ms\n",
            "787:\tlearn: 27.0568141\ttotal: 1.39s\tremaining: 375ms\n",
            "788:\tlearn: 27.0531438\ttotal: 1.39s\tremaining: 372ms\n",
            "789:\tlearn: 26.9963300\ttotal: 1.39s\tremaining: 370ms\n",
            "790:\tlearn: 26.9831134\ttotal: 1.39s\tremaining: 368ms\n",
            "791:\tlearn: 26.9574138\ttotal: 1.39s\tremaining: 366ms\n",
            "792:\tlearn: 26.9551593\ttotal: 1.4s\tremaining: 364ms\n",
            "793:\tlearn: 26.9480255\ttotal: 1.4s\tremaining: 362ms\n",
            "794:\tlearn: 26.9236698\ttotal: 1.4s\tremaining: 360ms\n",
            "795:\tlearn: 26.8904195\ttotal: 1.4s\tremaining: 358ms\n",
            "796:\tlearn: 26.8792780\ttotal: 1.4s\tremaining: 356ms\n",
            "797:\tlearn: 26.8649486\ttotal: 1.4s\tremaining: 354ms\n",
            "798:\tlearn: 26.8632270\ttotal: 1.4s\tremaining: 352ms\n",
            "799:\tlearn: 26.8294032\ttotal: 1.4s\tremaining: 350ms\n",
            "800:\tlearn: 26.8055260\ttotal: 1.4s\tremaining: 349ms\n",
            "801:\tlearn: 26.7859283\ttotal: 1.41s\tremaining: 348ms\n",
            "802:\tlearn: 26.7465155\ttotal: 1.41s\tremaining: 346ms\n",
            "803:\tlearn: 26.7387431\ttotal: 1.41s\tremaining: 344ms\n",
            "804:\tlearn: 26.7281566\ttotal: 1.41s\tremaining: 342ms\n",
            "805:\tlearn: 26.7112006\ttotal: 1.41s\tremaining: 340ms\n",
            "806:\tlearn: 26.6987645\ttotal: 1.41s\tremaining: 338ms\n",
            "807:\tlearn: 26.6660515\ttotal: 1.42s\tremaining: 336ms\n",
            "808:\tlearn: 26.6537894\ttotal: 1.42s\tremaining: 334ms\n",
            "809:\tlearn: 26.6261644\ttotal: 1.42s\tremaining: 333ms\n",
            "810:\tlearn: 26.6174819\ttotal: 1.42s\tremaining: 331ms\n",
            "811:\tlearn: 26.6165538\ttotal: 1.42s\tremaining: 329ms\n",
            "812:\tlearn: 26.6098623\ttotal: 1.42s\tremaining: 327ms\n",
            "813:\tlearn: 26.5976942\ttotal: 1.42s\tremaining: 325ms\n",
            "814:\tlearn: 26.5858177\ttotal: 1.42s\tremaining: 323ms\n",
            "815:\tlearn: 26.5511430\ttotal: 1.43s\tremaining: 322ms\n",
            "816:\tlearn: 26.4950945\ttotal: 1.43s\tremaining: 320ms\n",
            "817:\tlearn: 26.4933530\ttotal: 1.43s\tremaining: 318ms\n",
            "818:\tlearn: 26.4908140\ttotal: 1.43s\tremaining: 316ms\n",
            "819:\tlearn: 26.4754307\ttotal: 1.43s\tremaining: 314ms\n",
            "820:\tlearn: 26.4719221\ttotal: 1.43s\tremaining: 312ms\n",
            "821:\tlearn: 26.4678217\ttotal: 1.44s\tremaining: 311ms\n",
            "822:\tlearn: 26.4516682\ttotal: 1.44s\tremaining: 309ms\n",
            "823:\tlearn: 26.4218086\ttotal: 1.44s\tremaining: 307ms\n",
            "824:\tlearn: 26.4071133\ttotal: 1.44s\tremaining: 305ms\n",
            "825:\tlearn: 26.4059196\ttotal: 1.44s\tremaining: 303ms\n",
            "826:\tlearn: 26.3969359\ttotal: 1.45s\tremaining: 303ms\n",
            "827:\tlearn: 26.3789507\ttotal: 1.45s\tremaining: 301ms\n",
            "828:\tlearn: 26.3388175\ttotal: 1.45s\tremaining: 299ms\n",
            "829:\tlearn: 26.2995409\ttotal: 1.45s\tremaining: 297ms\n",
            "830:\tlearn: 26.2958006\ttotal: 1.45s\tremaining: 295ms\n",
            "831:\tlearn: 26.2888732\ttotal: 1.45s\tremaining: 293ms\n",
            "832:\tlearn: 26.2663056\ttotal: 1.45s\tremaining: 291ms\n",
            "833:\tlearn: 26.2240066\ttotal: 1.45s\tremaining: 289ms\n",
            "834:\tlearn: 26.1904801\ttotal: 1.45s\tremaining: 287ms\n",
            "835:\tlearn: 26.1783342\ttotal: 1.45s\tremaining: 285ms\n",
            "836:\tlearn: 26.1664388\ttotal: 1.45s\tremaining: 283ms\n",
            "837:\tlearn: 26.1655211\ttotal: 1.45s\tremaining: 281ms\n",
            "838:\tlearn: 26.1341441\ttotal: 1.46s\tremaining: 280ms\n",
            "839:\tlearn: 26.1219102\ttotal: 1.46s\tremaining: 278ms\n",
            "840:\tlearn: 26.1180702\ttotal: 1.46s\tremaining: 276ms\n",
            "841:\tlearn: 26.1152086\ttotal: 1.46s\tremaining: 274ms\n",
            "842:\tlearn: 26.0876837\ttotal: 1.46s\tremaining: 272ms\n",
            "843:\tlearn: 26.0768811\ttotal: 1.46s\tremaining: 270ms\n",
            "844:\tlearn: 26.0647802\ttotal: 1.47s\tremaining: 269ms\n",
            "845:\tlearn: 26.0239692\ttotal: 1.47s\tremaining: 267ms\n",
            "846:\tlearn: 26.0123822\ttotal: 1.47s\tremaining: 265ms\n",
            "847:\tlearn: 25.9776301\ttotal: 1.47s\tremaining: 263ms\n",
            "848:\tlearn: 25.9458810\ttotal: 1.47s\tremaining: 262ms\n",
            "849:\tlearn: 25.9217092\ttotal: 1.47s\tremaining: 260ms\n",
            "850:\tlearn: 25.9163578\ttotal: 1.47s\tremaining: 258ms\n",
            "851:\tlearn: 25.8930153\ttotal: 1.47s\tremaining: 256ms\n",
            "852:\tlearn: 25.8875179\ttotal: 1.47s\tremaining: 254ms\n",
            "853:\tlearn: 25.8719533\ttotal: 1.48s\tremaining: 252ms\n",
            "854:\tlearn: 25.8642314\ttotal: 1.48s\tremaining: 250ms\n",
            "855:\tlearn: 25.8594146\ttotal: 1.48s\tremaining: 248ms\n",
            "856:\tlearn: 25.8437830\ttotal: 1.48s\tremaining: 246ms\n",
            "857:\tlearn: 25.8278431\ttotal: 1.48s\tremaining: 245ms\n",
            "858:\tlearn: 25.8138621\ttotal: 1.48s\tremaining: 243ms\n",
            "859:\tlearn: 25.7890734\ttotal: 1.48s\tremaining: 241ms\n",
            "860:\tlearn: 25.7877652\ttotal: 1.49s\tremaining: 240ms\n",
            "861:\tlearn: 25.7825467\ttotal: 1.49s\tremaining: 238ms\n",
            "862:\tlearn: 25.7765803\ttotal: 1.49s\tremaining: 236ms\n",
            "863:\tlearn: 25.7680489\ttotal: 1.49s\tremaining: 234ms\n",
            "864:\tlearn: 25.7545947\ttotal: 1.49s\tremaining: 232ms\n",
            "865:\tlearn: 25.7475130\ttotal: 1.49s\tremaining: 231ms\n",
            "866:\tlearn: 25.7340325\ttotal: 1.49s\tremaining: 229ms\n",
            "867:\tlearn: 25.7035216\ttotal: 1.49s\tremaining: 227ms\n",
            "868:\tlearn: 25.6945005\ttotal: 1.49s\tremaining: 225ms\n",
            "869:\tlearn: 25.6841399\ttotal: 1.49s\tremaining: 223ms\n",
            "870:\tlearn: 25.6790570\ttotal: 1.5s\tremaining: 221ms\n",
            "871:\tlearn: 25.6366822\ttotal: 1.5s\tremaining: 220ms\n",
            "872:\tlearn: 25.6048733\ttotal: 1.5s\tremaining: 218ms\n",
            "873:\tlearn: 25.5872424\ttotal: 1.5s\tremaining: 216ms\n",
            "874:\tlearn: 25.5703655\ttotal: 1.5s\tremaining: 215ms\n",
            "875:\tlearn: 25.5683239\ttotal: 1.5s\tremaining: 213ms\n",
            "876:\tlearn: 25.5634946\ttotal: 1.51s\tremaining: 211ms\n",
            "877:\tlearn: 25.5167081\ttotal: 1.51s\tremaining: 210ms\n",
            "878:\tlearn: 25.5156596\ttotal: 1.51s\tremaining: 208ms\n",
            "879:\tlearn: 25.5015485\ttotal: 1.51s\tremaining: 206ms\n",
            "880:\tlearn: 25.4865146\ttotal: 1.51s\tremaining: 204ms\n",
            "881:\tlearn: 25.4808186\ttotal: 1.51s\tremaining: 202ms\n",
            "882:\tlearn: 25.4620896\ttotal: 1.52s\tremaining: 201ms\n",
            "883:\tlearn: 25.4464024\ttotal: 1.52s\tremaining: 199ms\n",
            "884:\tlearn: 25.4346015\ttotal: 1.52s\tremaining: 197ms\n",
            "885:\tlearn: 25.4287886\ttotal: 1.52s\tremaining: 195ms\n",
            "886:\tlearn: 25.4213892\ttotal: 1.52s\tremaining: 193ms\n",
            "887:\tlearn: 25.3981423\ttotal: 1.52s\tremaining: 192ms\n",
            "888:\tlearn: 25.3606155\ttotal: 1.52s\tremaining: 190ms\n",
            "889:\tlearn: 25.3353212\ttotal: 1.52s\tremaining: 188ms\n",
            "890:\tlearn: 25.3264288\ttotal: 1.52s\tremaining: 186ms\n",
            "891:\tlearn: 25.2910609\ttotal: 1.52s\tremaining: 184ms\n",
            "892:\tlearn: 25.2820435\ttotal: 1.53s\tremaining: 183ms\n",
            "893:\tlearn: 25.2802531\ttotal: 1.53s\tremaining: 181ms\n",
            "894:\tlearn: 25.2763441\ttotal: 1.53s\tremaining: 180ms\n",
            "895:\tlearn: 25.2586327\ttotal: 1.53s\tremaining: 178ms\n",
            "896:\tlearn: 25.2537435\ttotal: 1.53s\tremaining: 176ms\n",
            "897:\tlearn: 25.2203833\ttotal: 1.53s\tremaining: 174ms\n",
            "898:\tlearn: 25.2156394\ttotal: 1.53s\tremaining: 172ms\n",
            "899:\tlearn: 25.2122652\ttotal: 1.53s\tremaining: 171ms\n",
            "900:\tlearn: 25.2048805\ttotal: 1.54s\tremaining: 169ms\n",
            "901:\tlearn: 25.1883905\ttotal: 1.54s\tremaining: 168ms\n",
            "902:\tlearn: 25.1816728\ttotal: 1.54s\tremaining: 166ms\n",
            "903:\tlearn: 25.1699374\ttotal: 1.54s\tremaining: 164ms\n",
            "904:\tlearn: 25.1491599\ttotal: 1.54s\tremaining: 162ms\n",
            "905:\tlearn: 25.1359992\ttotal: 1.54s\tremaining: 160ms\n",
            "906:\tlearn: 25.1337859\ttotal: 1.54s\tremaining: 158ms\n",
            "907:\tlearn: 25.1176905\ttotal: 1.55s\tremaining: 157ms\n",
            "908:\tlearn: 25.0903244\ttotal: 1.55s\tremaining: 155ms\n",
            "909:\tlearn: 25.0800588\ttotal: 1.55s\tremaining: 154ms\n",
            "910:\tlearn: 25.0719324\ttotal: 1.55s\tremaining: 152ms\n",
            "911:\tlearn: 25.0516062\ttotal: 1.55s\tremaining: 150ms\n",
            "912:\tlearn: 25.0353680\ttotal: 1.55s\tremaining: 148ms\n",
            "913:\tlearn: 24.9712920\ttotal: 1.56s\tremaining: 147ms\n",
            "914:\tlearn: 24.9633404\ttotal: 1.56s\tremaining: 145ms\n",
            "915:\tlearn: 24.9586692\ttotal: 1.56s\tremaining: 143ms\n",
            "916:\tlearn: 24.9569762\ttotal: 1.56s\tremaining: 141ms\n",
            "917:\tlearn: 24.9555364\ttotal: 1.56s\tremaining: 140ms\n",
            "918:\tlearn: 24.9492172\ttotal: 1.56s\tremaining: 138ms\n",
            "919:\tlearn: 24.9099324\ttotal: 1.57s\tremaining: 136ms\n",
            "920:\tlearn: 24.9030976\ttotal: 1.57s\tremaining: 135ms\n",
            "921:\tlearn: 24.8969278\ttotal: 1.57s\tremaining: 133ms\n",
            "922:\tlearn: 24.8951263\ttotal: 1.57s\tremaining: 131ms\n",
            "923:\tlearn: 24.8664348\ttotal: 1.57s\tremaining: 129ms\n",
            "924:\tlearn: 24.8640326\ttotal: 1.57s\tremaining: 127ms\n",
            "925:\tlearn: 24.8609958\ttotal: 1.58s\tremaining: 126ms\n",
            "926:\tlearn: 24.8523139\ttotal: 1.58s\tremaining: 124ms\n",
            "927:\tlearn: 24.8502050\ttotal: 1.58s\tremaining: 122ms\n",
            "928:\tlearn: 24.8487067\ttotal: 1.58s\tremaining: 121ms\n",
            "929:\tlearn: 24.8342425\ttotal: 1.58s\tremaining: 119ms\n",
            "930:\tlearn: 24.7979971\ttotal: 1.58s\tremaining: 117ms\n",
            "931:\tlearn: 24.7822687\ttotal: 1.59s\tremaining: 116ms\n",
            "932:\tlearn: 24.7680187\ttotal: 1.59s\tremaining: 114ms\n",
            "933:\tlearn: 24.7671638\ttotal: 1.6s\tremaining: 113ms\n",
            "934:\tlearn: 24.7458598\ttotal: 1.6s\tremaining: 111ms\n",
            "935:\tlearn: 24.7443095\ttotal: 1.61s\tremaining: 110ms\n",
            "936:\tlearn: 24.7387178\ttotal: 1.61s\tremaining: 108ms\n",
            "937:\tlearn: 24.7121209\ttotal: 1.62s\tremaining: 107ms\n",
            "938:\tlearn: 24.6851282\ttotal: 1.62s\tremaining: 105ms\n",
            "939:\tlearn: 24.6742425\ttotal: 1.62s\tremaining: 103ms\n",
            "940:\tlearn: 24.6605403\ttotal: 1.62s\tremaining: 102ms\n",
            "941:\tlearn: 24.6499976\ttotal: 1.62s\tremaining: 100ms\n",
            "942:\tlearn: 24.6354409\ttotal: 1.63s\tremaining: 98.3ms\n",
            "943:\tlearn: 24.6218005\ttotal: 1.63s\tremaining: 96.9ms\n",
            "944:\tlearn: 24.6135050\ttotal: 1.63s\tremaining: 95.1ms\n",
            "945:\tlearn: 24.5812552\ttotal: 1.64s\tremaining: 93.4ms\n",
            "946:\tlearn: 24.5575592\ttotal: 1.64s\tremaining: 91.6ms\n",
            "947:\tlearn: 24.5375476\ttotal: 1.64s\tremaining: 89.9ms\n",
            "948:\tlearn: 24.5221901\ttotal: 1.64s\tremaining: 88.1ms\n",
            "949:\tlearn: 24.5192013\ttotal: 1.65s\tremaining: 86.7ms\n",
            "950:\tlearn: 24.5063525\ttotal: 1.65s\tremaining: 84.9ms\n",
            "951:\tlearn: 24.4874955\ttotal: 1.65s\tremaining: 83.2ms\n",
            "952:\tlearn: 24.4864669\ttotal: 1.65s\tremaining: 81.4ms\n",
            "953:\tlearn: 24.4768324\ttotal: 1.65s\tremaining: 79.7ms\n",
            "954:\tlearn: 24.4720487\ttotal: 1.65s\tremaining: 77.9ms\n",
            "955:\tlearn: 24.4665783\ttotal: 1.66s\tremaining: 76.3ms\n",
            "956:\tlearn: 24.4583901\ttotal: 1.66s\tremaining: 74.7ms\n",
            "957:\tlearn: 24.4479896\ttotal: 1.66s\tremaining: 72.9ms\n",
            "958:\tlearn: 24.4423739\ttotal: 1.67s\tremaining: 71.2ms\n",
            "959:\tlearn: 24.4331674\ttotal: 1.67s\tremaining: 69.7ms\n",
            "960:\tlearn: 24.4156252\ttotal: 1.67s\tremaining: 68ms\n",
            "961:\tlearn: 24.4053492\ttotal: 1.68s\tremaining: 66.2ms\n",
            "962:\tlearn: 24.3649981\ttotal: 1.68s\tremaining: 64.5ms\n",
            "963:\tlearn: 24.3566472\ttotal: 1.68s\tremaining: 62.7ms\n",
            "964:\tlearn: 24.3212519\ttotal: 1.68s\tremaining: 61ms\n",
            "965:\tlearn: 24.3027147\ttotal: 1.69s\tremaining: 59.4ms\n",
            "966:\tlearn: 24.2884130\ttotal: 1.69s\tremaining: 57.7ms\n",
            "967:\tlearn: 24.2835154\ttotal: 1.69s\tremaining: 55.9ms\n",
            "968:\tlearn: 24.2745555\ttotal: 1.69s\tremaining: 54.1ms\n",
            "969:\tlearn: 24.2421412\ttotal: 1.69s\tremaining: 52.4ms\n",
            "970:\tlearn: 24.2099918\ttotal: 1.69s\tremaining: 50.6ms\n",
            "971:\tlearn: 24.1967081\ttotal: 1.69s\tremaining: 48.8ms\n",
            "972:\tlearn: 24.1694116\ttotal: 1.7s\tremaining: 47.1ms\n",
            "973:\tlearn: 24.1607056\ttotal: 1.7s\tremaining: 45.4ms\n",
            "974:\tlearn: 24.1283313\ttotal: 1.71s\tremaining: 43.8ms\n",
            "975:\tlearn: 24.1220861\ttotal: 1.71s\tremaining: 42ms\n",
            "976:\tlearn: 24.1029233\ttotal: 1.71s\tremaining: 40.3ms\n",
            "977:\tlearn: 24.0674178\ttotal: 1.72s\tremaining: 38.6ms\n",
            "978:\tlearn: 24.0409824\ttotal: 1.72s\tremaining: 36.8ms\n",
            "979:\tlearn: 24.0262550\ttotal: 1.72s\tremaining: 35.1ms\n",
            "980:\tlearn: 24.0207384\ttotal: 1.72s\tremaining: 33.3ms\n",
            "981:\tlearn: 24.0181070\ttotal: 1.73s\tremaining: 31.7ms\n",
            "982:\tlearn: 24.0108050\ttotal: 1.73s\tremaining: 29.9ms\n",
            "983:\tlearn: 24.0023740\ttotal: 1.73s\tremaining: 28.1ms\n",
            "984:\tlearn: 23.9920646\ttotal: 1.73s\tremaining: 26.3ms\n",
            "985:\tlearn: 23.9887926\ttotal: 1.74s\tremaining: 24.7ms\n",
            "986:\tlearn: 23.9712422\ttotal: 1.74s\tremaining: 22.9ms\n",
            "987:\tlearn: 23.9479922\ttotal: 1.74s\tremaining: 21.1ms\n",
            "988:\tlearn: 23.9405005\ttotal: 1.74s\tremaining: 19.4ms\n",
            "989:\tlearn: 23.9347218\ttotal: 1.74s\tremaining: 17.6ms\n",
            "990:\tlearn: 23.9213892\ttotal: 1.75s\tremaining: 15.9ms\n",
            "991:\tlearn: 23.9069487\ttotal: 1.75s\tremaining: 14.1ms\n",
            "992:\tlearn: 23.8991742\ttotal: 1.75s\tremaining: 12.4ms\n",
            "993:\tlearn: 23.8895459\ttotal: 1.75s\tremaining: 10.6ms\n",
            "994:\tlearn: 23.8532299\ttotal: 1.75s\tremaining: 8.82ms\n",
            "995:\tlearn: 23.8432187\ttotal: 1.76s\tremaining: 7.05ms\n",
            "996:\tlearn: 23.8346126\ttotal: 1.77s\tremaining: 5.32ms\n",
            "997:\tlearn: 23.8162288\ttotal: 1.77s\tremaining: 3.54ms\n",
            "998:\tlearn: 23.7769151\ttotal: 1.77s\tremaining: 1.77ms\n",
            "999:\tlearn: 23.7632376\ttotal: 1.77s\tremaining: 0us\n",
            "Root Mean Squared Error on test set: 70.34148852784013\n",
            "          Date  predicted_sales\n",
            "0   2023-01-01       339.691633\n",
            "1   2023-01-02       336.497245\n",
            "2   2023-01-03       326.948791\n",
            "3   2023-01-04       313.430252\n",
            "4   2023-01-05       309.627762\n",
            "..         ...              ...\n",
            "360 2023-12-27       267.316324\n",
            "361 2023-12-28       324.323542\n",
            "362 2023-12-29       343.629096\n",
            "363 2023-12-30       321.470218\n",
            "364 2023-12-31       302.612235\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_18efa7a9-e95d-4e86-bb13-264194e17ce1\", \"catboostnext_year_data.csv\", 17556)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.***PCA -principal-componenet-analysis-***"
      ],
      "metadata": {
        "id": "AkRTJtN8feyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y = train_data['Sales']\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "n_components = 5  # Choose the number of principal components\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Initialize a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the entire dataset\n",
        "model.fit(X_pca, y)\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Apply PCA to the next year data\n",
        "next_year_data_pca = pca.transform(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data_pca)\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "\n",
        "next_year_data.to_csv('pcanext_year_data.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "files.download('pcanext_year_data.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "6PZLCnEde746",
        "outputId": "607a099b-d780-45b8-99bd-abb361b72ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date  predicted_sales\n",
            "0   2023-01-01       337.190690\n",
            "1   2023-01-02       312.628695\n",
            "2   2023-01-03       312.404302\n",
            "3   2023-01-04       312.179909\n",
            "4   2023-01-05       311.955516\n",
            "..         ...              ...\n",
            "360 2023-12-27       293.294964\n",
            "361 2023-12-28       293.070572\n",
            "362 2023-12-29       292.846179\n",
            "363 2023-12-30       292.621786\n",
            "364 2023-12-31       292.397393\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a70c71eb-e03f-4373-9279-930c1932df2e\", \"pcanext_year_data.csv\", 17545)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.ICA-independent-component-analysis"
      ],
      "metadata": {
        "id": "YTdmg_NcgNTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load your training data\n",
        "train_data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTSzRmxv_yRbTSGE-UfeHLFhvPTiaEMdq8hUbsovAvvo4PNdZYqxOsO9zdVOiMlC1Wperm9rrTbhibW/pub?output=csv')\n",
        "\n",
        "# Ensure that the 'Date' column is in the correct date format\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "# Feature engineering for the training data (similar to previous code)\n",
        "train_data['year'] = train_data['Date'].dt.year\n",
        "train_data['month'] = train_data['Date'].dt.month\n",
        "train_data['day'] = train_data['Date'].dt.day\n",
        "train_data['day_of_week'] = train_data['Date'].dt.dayofweek\n",
        "train_data['day_of_year'] = train_data['Date'].dt.dayofyear\n",
        "train_data['week_of_year'] = train_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Drop the original 'Date' column and target 'Sales' column\n",
        "X = train_data.drop(['Date', 'Sales'], axis=1)\n",
        "y = train_data['Sales']\n",
        "\n",
        "# Apply ICA to reduce dimensionality\n",
        "n_components = 5  # Choose the number of independent components\n",
        "ica = FastICA(n_components=n_components, random_state=42)\n",
        "X_ica = ica.fit_transform(X)\n",
        "\n",
        "# Initialize a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the entire dataset\n",
        "model.fit(X_ica, y)\n",
        "\n",
        "# Generate a DataFrame for the next year\n",
        "next_year_dates = pd.date_range(start=train_data['Date'].max() + timedelta(days=1), periods=365, freq='D')\n",
        "next_year_data = pd.DataFrame({'Date': next_year_dates})\n",
        "\n",
        "# Feature engineering for the next year data\n",
        "next_year_data['year'] = next_year_data['Date'].dt.year\n",
        "next_year_data['month'] = next_year_data['Date'].dt.month\n",
        "next_year_data['day'] = next_year_data['Date'].dt.day\n",
        "next_year_data['day_of_week'] = next_year_data['Date'].dt.dayofweek\n",
        "next_year_data['day_of_year'] = next_year_data['Date'].dt.dayofyear\n",
        "next_year_data['week_of_year'] = next_year_data['Date'].dt.isocalendar().week\n",
        "\n",
        "# Apply ICA to the next year data\n",
        "next_year_data_ica = ica.transform(next_year_data.drop('Date', axis=1))\n",
        "\n",
        "# Use the trained model to predict sales for the next year\n",
        "next_year_predictions = model.predict(next_year_data_ica)\n",
        "\n",
        "# Add the predicted values to the next year DataFrame\n",
        "next_year_data['predicted_sales'] = next_year_predictions\n",
        "\n",
        "# Print the DataFrame with 'Date' and 'predicted_sales' columns\n",
        "print(next_year_data[['Date', 'predicted_sales']])\n",
        "\n",
        "\n",
        "next_year_data.to_csv('icanext_year_data.csv',index=False)\n",
        "\n",
        "\n",
        "files.download('icanext_year_data.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "wuxjjRH0e70F",
        "outputId": "e721c43f-73f6-41c7-bf41-237c89c01dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date  predicted_sales\n",
            "0   2023-01-01       304.650560\n",
            "1   2023-01-02       313.459909\n",
            "2   2023-01-03       313.279787\n",
            "3   2023-01-04       313.099665\n",
            "4   2023-01-05       312.919542\n",
            "..         ...              ...\n",
            "360 2023-12-27       292.409479\n",
            "361 2023-12-28       292.229357\n",
            "362 2023-12-29       292.049235\n",
            "363 2023-12-30       291.869112\n",
            "364 2023-12-31       291.688990\n",
            "\n",
            "[365 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5565441e-1096-4abf-8860-c2c88e45152f\", \"icanext_year_data.csv\", 17527)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lY76OcGhkdML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDS5xmIMkdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XznuIZjqkdT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###project 1.Datacollection\n",
        "2.Model Iteration(Create large amount of models)\n",
        "3.Intermediate model creation\n",
        "4.Fine tuning and predictive model development\n",
        "5.Documenting and reporting"
      ],
      "metadata": {
        "id": "h-K1VFackdXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjuzmHBpe7xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jZ49ykD7e7vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5sh7MTPe7ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FWKCz8t9e7qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-HSwNxCe7kP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}